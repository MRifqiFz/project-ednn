{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchmetrics.functional.classification import *\n",
    "from skorch import NeuralNetClassifier\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Dropout, InputSpec\n",
    "from keras.utils import *\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "from skmultilearn.ext import Keras\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X24</th>\n",
       "      <th>X25</th>\n",
       "      <th>X26</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Y3</th>\n",
       "      <th>Y4</th>\n",
       "      <th>Y5</th>\n",
       "      <th>Y6</th>\n",
       "      <th>Y7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1  X2  X3  X4  X5  X6  X7  X8  X9  X10  ...  X24  X25  X26  Y1  Y2  Y3  \\\n",
       "0   3   1   1   1   1   1   1   1   3    1  ...    1    3    1   2   2   1   \n",
       "1   5   3   1   1   2   4   2   1   5    1  ...    1    2    2   2   2   2   \n",
       "2   4   1   1   1   2   1   2   2   2    2  ...    2    3    1   2   2   2   \n",
       "3   4   1   1   1   2   4   2   1   1    1  ...    2    3    2   2   2   2   \n",
       "4   2   1   1   3   1   3   1   1   1    1  ...    1    3    1   2   2   2   \n",
       "\n",
       "   Y4  Y5  Y6  Y7  \n",
       "0   2   2   1   2  \n",
       "1   2   2   1   1  \n",
       "2   2   2   2   1  \n",
       "3   1   2   1   2  \n",
       "4   2   2   2   2  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('D:\\Project\\project-ednn\\Code\\Final_dataset_Diabetes_Complication.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X24</th>\n",
       "      <th>X25</th>\n",
       "      <th>X26</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Y3</th>\n",
       "      <th>Y4</th>\n",
       "      <th>Y5</th>\n",
       "      <th>Y6</th>\n",
       "      <th>Y7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543770</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543771</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543772</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543773</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543774</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543775 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        X1  X2  X3  X4  X5  X6  X7  X8  X9  X10  ...  X24  X25  X26  Y1  Y2  \\\n",
       "0        3   1   1   1   1   1   1   1   3    1  ...    1    3    1   2   2   \n",
       "1        5   3   1   1   2   4   2   1   5    1  ...    1    2    2   2   2   \n",
       "2        4   1   1   1   2   1   2   2   2    2  ...    2    3    1   2   2   \n",
       "3        4   1   1   1   2   4   2   1   1    1  ...    2    3    2   2   2   \n",
       "4        2   1   1   3   1   3   1   1   1    1  ...    1    3    1   2   2   \n",
       "...     ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...  ...  ...  ..  ..   \n",
       "543770   1   1   2   1   1   4   1   1   5    2  ...    2    3    1   2   2   \n",
       "543771   2   1   1   1   1   4   1   1   5    1  ...    1    2    2   2   2   \n",
       "543772   1   2   1   1   1   3   1   1   2    1  ...    1    3    1   2   2   \n",
       "543773   3   1   2   1   1   3   1   1   2    2  ...    1    3    2   2   2   \n",
       "543774   1   1   1   1   1   4   1   1   2    1  ...    1    3    2   2   2   \n",
       "\n",
       "        Y3  Y4  Y5  Y6  Y7  \n",
       "0        1   2   2   1   2  \n",
       "1        2   2   2   1   1  \n",
       "2        2   2   2   2   1  \n",
       "3        2   1   2   1   2  \n",
       "4        2   2   2   2   2  \n",
       "...     ..  ..  ..  ..  ..  \n",
       "543770   2   2   2   2   2  \n",
       "543771   1   2   2   2   1  \n",
       "543772   2   2   2   2   1  \n",
       "543773   2   2   1   2   2  \n",
       "543774   2   2   2   2   2  \n",
       "\n",
       "[543775 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop_duplicates(ignore_index=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 543775 entries, 0 to 543774\n",
      "Data columns (total 33 columns):\n",
      " #   Column  Non-Null Count   Dtype\n",
      "---  ------  --------------   -----\n",
      " 0   X1      543775 non-null  int64\n",
      " 1   X2      543775 non-null  int64\n",
      " 2   X3      543775 non-null  int64\n",
      " 3   X4      543775 non-null  int64\n",
      " 4   X5      543775 non-null  int64\n",
      " 5   X6      543775 non-null  int64\n",
      " 6   X7      543775 non-null  int64\n",
      " 7   X8      543775 non-null  int64\n",
      " 8   X9      543775 non-null  int64\n",
      " 9   X10     543775 non-null  int64\n",
      " 10  X11     543775 non-null  int64\n",
      " 11  X12     543775 non-null  int64\n",
      " 12  X13     543775 non-null  int64\n",
      " 13  X14     543775 non-null  int64\n",
      " 14  X15     543775 non-null  int64\n",
      " 15  X16     543775 non-null  int64\n",
      " 16  X17     543775 non-null  int64\n",
      " 17  X18     543775 non-null  int64\n",
      " 18  X19     543775 non-null  int64\n",
      " 19  X20     543775 non-null  int64\n",
      " 20  X21     543775 non-null  int64\n",
      " 21  X22     543775 non-null  int64\n",
      " 22  X23     543775 non-null  int64\n",
      " 23  X24     543775 non-null  int64\n",
      " 24  X25     543775 non-null  int64\n",
      " 25  X26     543775 non-null  int64\n",
      " 26  Y1      543775 non-null  int64\n",
      " 27  Y2      543775 non-null  int64\n",
      " 28  Y3      543775 non-null  int64\n",
      " 29  Y4      543775 non-null  int64\n",
      " 30  Y5      543775 non-null  int64\n",
      " 31  Y6      543775 non-null  int64\n",
      " 32  Y7      543775 non-null  int64\n",
      "dtypes: int64(33)\n",
      "memory usage: 136.9 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Y3</th>\n",
       "      <th>Y4</th>\n",
       "      <th>Y5</th>\n",
       "      <th>Y6</th>\n",
       "      <th>Y7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543770</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543771</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543772</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543773</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543774</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>543775 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Y1  Y2  Y3  Y4  Y5  Y6  Y7\n",
       "0        2   2   1   2   2   1   2\n",
       "1        2   2   2   2   2   1   1\n",
       "2        2   2   2   2   2   2   1\n",
       "3        2   2   2   1   2   1   2\n",
       "4        2   2   2   2   2   2   2\n",
       "...     ..  ..  ..  ..  ..  ..  ..\n",
       "543770   2   2   2   2   2   2   2\n",
       "543771   2   2   1   2   2   2   1\n",
       "543772   2   2   2   2   2   2   1\n",
       "543773   2   2   2   2   1   2   2\n",
       "543774   2   2   2   2   2   2   2\n",
       "\n",
       "[543775 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feature = data.iloc[:, :26]\n",
    "data_label = data.iloc[:, 26:]\n",
    "data_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "\n",
    "# Transform the DataFrame to obtain the normalized data\n",
    "data_normalized = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_normalized[:, :26]\n",
    "y = data_normalized[:, 26:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalize = pd.DataFrame(scaler.transform(data), columns=data.columns)\n",
    "df_feature = data_normalize.iloc[:, :26]\n",
    "df_label = data_normalize.iloc[:, 26:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list = []\n",
    "for i, label in enumerate(df_label.columns):\n",
    "    values_1 = df_label[label][df_label[label] == 1]\n",
    "    values_2 = df_label[label][df_label[label] == 2]\n",
    "    dist = 1 - (len(values_1) / len(df_label))\n",
    "    weight_list.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0483, 0.0652, 0.0605, 0.0508, 0.1008, 0.3895, 0.4290],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_ = torch.Tensor(weight_list).to(device)\n",
    "weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    for layer in m.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            # print(f'Reset trainable parameters of layer = {layer}')\n",
    "            layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModelwithWeightedDropout(tf.keras.Model):\n",
    "    def __init__(self, num_features=26, num_classes=7, p=[0.05, 0.01, 0.003]):\n",
    "        super(DNNModelwithWeightedDropout, self).__init__()\n",
    "        self.fcn1 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, activation='relu', input_shape=(num_features,)),\n",
    "            tf.keras.layers.Dense(len(p), activation='softmax')\n",
    "        ])\n",
    "        self.dropout = [tf.keras.layers.Dropout(rate=rate) for rate in p]\n",
    "        self.fcn2 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        x = self.fcn1(x)\n",
    "        weights = x\n",
    "\n",
    "        # Apply weighted dropout\n",
    "        x = [dropout(x[:, i], training=training) * weights[:, i] for i, dropout in enumerate(self.dropout)]\n",
    "\n",
    "        x = tf.stack(x, axis=1)\n",
    "        x = self.fcn2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(tf.keras.Model):\n",
    "    def __init__(self, num_features=26, num_classes=7):\n",
    "        super(DNNModel, self).__init__()\n",
    "        self.fcn1 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, activation='relu', input_shape=(num_features,)),\n",
    "            tf.keras.layers.Dense(3, activation='softmax')\n",
    "        ])\n",
    "        self.fcn2 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=None, mask=None):\n",
    "        x = self.fcn1(x)\n",
    "        x = self.fcn2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNDropout(tf.keras.Model):\n",
    "    def __init__(self, num_features=26, num_classes=7, p=0.5):\n",
    "        super(DNNDropout, self).__init__()\n",
    "\n",
    "        self.fcn1 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(8, activation='relu'),\n",
    "            tf.keras.layers.Dense(3, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(p)\n",
    "\n",
    "        self.fcn2 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.fcn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fcn2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNWeightedDropout(tf.keras.Model):\n",
    "    def __init__(self, num_features=26, num_classes=7, p=0.5):\n",
    "        super(DNNWeightedDropout, self).__init__()\n",
    "\n",
    "        self.fcn1 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(8, activation='relu'),\n",
    "            tf.keras.layers.Dense(3, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(p)\n",
    "\n",
    "        self.fcn2 = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.fcn1(x)\n",
    "        weights = x\n",
    "        x = self.dropout1(x) * weights\n",
    "        x = self.fcn2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(n_inputs, n_outputs):\n",
    "    model = DNNModel(num_features=n_inputs, num_classes=n_outputs)\n",
    "    lossWeights = weight_list\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.003), metrics=['accuracy'], loss_weights=lossWeights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DropoutModel(n_inputs, n_outputs, dropout_rate=0.3):\n",
    "    model = DNNDropout(num_features=n_inputs, num_classes=n_outputs, p=dropout_rate)\n",
    "    lossWeights = weight_list\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.003), metrics=['accuracy'], loss_weights=lossWeights)\n",
    "\n",
    "    return model\n",
    "\n",
    "def WeightedDropoutModel(n_inputs, n_outputs, dropout_rate=0.3):\n",
    "    model = DNNWeightedDropout(num_features=n_inputs, num_classes=n_outputs, p=dropout_rate)\n",
    "    lossWeights = weight_list\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.003), metrics=['accuracy'], loss_weights=lossWeights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProposedModel1(n_inputs, n_outputs, dropout_rate=[0.3]*3):\n",
    "    model = DNNModelwithWeightedDropout(num_features=n_inputs, num_classes=n_outputs, p=dropout_rate)\n",
    "    lossWeights = weight_list\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.003), metrics=['accuracy'], loss_weights=lossWeights)\n",
    "\n",
    "    return model\n",
    "\n",
    "def ProposedModel2(n_inputs, n_outputs, dropout_rate=[0.1, 0.2, 0.3]):\n",
    "    model = DNNModelwithWeightedDropout(num_features=n_inputs, num_classes=n_outputs, p=dropout_rate)\n",
    "    lossWeights = weight_list\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.003), metrics=['accuracy'], loss_weights=lossWeights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LPDropoutModel(n_inputs=26, n_outputs=7):\n",
    "    model = DNNDropout(num_features=n_inputs, num_classes=n_outputs, p=0.3)\n",
    "    lossWeights = weight_list\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def LPWeightedDropoutModel(n_inputs=26, n_outputs=7):\n",
    "    model = DNNWeightedDropout(num_features=n_inputs, num_classes=n_outputs, p=0.3)\n",
    "    lossWeights = weight_list\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def LPProposedModel1(n_inputs=26, n_outputs=7):\n",
    "    model = DNNModelwithWeightedDropout(num_features=n_inputs, num_classes=n_outputs, p=[0.3]*3)\n",
    "    lossWeights = weight_list\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def LPProposedModel2(n_inputs=26, n_outputs=7):\n",
    "    model = DNNModelwithWeightedDropout(num_features=n_inputs, num_classes=n_outputs, p=[0.1, 0.2, 0.3])\n",
    "    lossWeights = weight_list\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DNN(26, 7)\n",
    "\n",
    "# train_init = time.time()\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "# train_time = time.time() - train_init\n",
    "\n",
    "# test_init = time.time()\n",
    "# y_pred = model.predict(x_test)\n",
    "# test_time = time.time() - test_init\n",
    "\n",
    "# dnn_trues = torch.tensor(y_test)\n",
    "# dnn_preds = torch.tensor(y_pred)\n",
    "# print(f\"Accuracy: {multilabel_accuracy(dnn_preds, dnn_trues, num_labels=7, average='micro').item():.5f} | Training Time: {train_time:.5f} | Testing Time: {test_time:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DropoutModel(26, 7, dropout_rate=0.3)\n",
    "\n",
    "# train_init = time.time()\n",
    "# model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "# train_time = time.time() - train_init\n",
    "\n",
    "# test_init = time.time()\n",
    "# y_pred = model.predict(x_test)\n",
    "# test_time = time.time() - test_init\n",
    "\n",
    "# do_trues = torch.tensor(y_test)\n",
    "# do_preds = torch.tensor(y_pred)\n",
    "# print(f\"Accuracy: {multilabel_accuracy(dnn_preds, dnn_trues, num_labels=7, average='micro').item():.5f} | Training Time: {train_time:.5f} | Testing Time: {test_time:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = [3, 8, 16, 32, 64, 128]\n",
    "# pm_trues = []\n",
    "# pm_preds = []\n",
    "# for i in n:\n",
    "#     d_rate = [0.3] * i\n",
    "#     model = ProposedModel(26, 7, dropout_rate=d_rate)\n",
    "\n",
    "#     train_init = time.time()\n",
    "#     model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "#     train_time = time.time() - train_init\n",
    "\n",
    "#     test_init = time.time()\n",
    "#     y_pred = model.predict(x_test)\n",
    "#     test_time = time.time() - test_init\n",
    "\n",
    "#     temp_trues = torch.tensor(y_test)\n",
    "#     temp_preds = torch.tensor(y_pred)\n",
    "\n",
    "#     pm_trues.append(temp_trues)\n",
    "#     pm_preds.append(temp_preds)\n",
    "\n",
    "#     print(f\"Number Dropout: {i} | Accuracy: {multilabel_accuracy(temp_preds, temp_trues, num_labels=7, average='micro').item():.5f} | Training Time: {train_time:.5f} | Testing Time: {test_time:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_based(trues, preds):\n",
    "    acc_micro = multilabel_accuracy(preds, trues, num_labels=7, average='micro').item()\n",
    "    acc_macro = multilabel_accuracy(preds, trues, num_labels=7, average='macro').item()\n",
    "    prec_micro = multilabel_precision(preds, trues, num_labels=7, average='micro').item()\n",
    "    prec_macro = multilabel_precision(preds, trues, num_labels=7, average='macro').item()\n",
    "    rec_micro = multilabel_recall(preds, trues, num_labels=7, average='micro').item()\n",
    "    rec_macro = multilabel_recall(preds, trues, num_labels=7, average='macro').item()\n",
    "    f1_micro = multilabel_f1_score(preds, trues, num_labels=7, average='micro').item()\n",
    "    f1_macro = multilabel_f1_score(preds, trues, num_labels=7, average='macro').item()\n",
    "    auc_micro = multilabel_auroc(preds.float(), trues.long(), num_labels=7, average='micro').item()\n",
    "    auc_macro = multilabel_auroc(preds.float(), trues.long(), num_labels=7, average='macro').item()\n",
    "\n",
    "    data = {\n",
    "        \"Accuracy (Micro)\": acc_micro,\n",
    "        \"Accuracy (Macro)\": acc_macro,\n",
    "        \"Precision (Micro)\": prec_micro,\n",
    "        \"Precision (Macro)\": prec_macro,\n",
    "        \"Recall (Micro)\": rec_micro,\n",
    "        \"Recall (Macro)\": rec_macro,\n",
    "        \"F1 Score (Micro)\": f1_micro,\n",
    "        \"F1 Score (Macro)\": f1_macro,\n",
    "        \"AUC (Micro)\": auc_micro,\n",
    "        \"AUC (Macro)\": auc_macro\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([data])  # Creating a DataFrame with a single row\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sci\n",
    "\n",
    "def findmax(outputs):\n",
    "    if len(outputs) == 0:\n",
    "        return None, None  # Handle the case when outputs is empty\n",
    "\n",
    "    Max = -float(\"inf\")\n",
    "    index = 0\n",
    "\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i].float() > Max:\n",
    "            Max = outputs[i]\n",
    "            index = i\n",
    "\n",
    "    return Max, index\n",
    "\n",
    "def OneError(outputs, test_target):\n",
    "    test_data_num = outputs.shape[0]\n",
    "    class_num = outputs.shape[1]\n",
    "    num = 0\n",
    "    one_error = 0\n",
    "    for i in range(test_data_num):\n",
    "        if sum(test_target[i]) != class_num and sum(test_target[i]) != 0:\n",
    "            Max, index = findmax(outputs[i])\n",
    "            num = num + 1\n",
    "            if test_target[i][index] != 1:\n",
    "                one_error = one_error + 1\n",
    "    return one_error / num\n",
    "\n",
    "def Accuracy(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        x = sum(np.logical_and(y_true[i], y_pred[i]))\n",
    "        y = sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "        temp +=  x/y if not np.isnan(x/y) else 0\n",
    "    return temp / y_true.shape[0]\n",
    "\n",
    "def Recall(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if sum(y_true[i]) == 0:\n",
    "            continue\n",
    "        temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_true[i])\n",
    "    return temp/ y_true.shape[0]\n",
    "\n",
    "def Precision(y_true, y_pred):\n",
    "    temp = 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if sum(y_pred[i]) == 0:\n",
    "            continue\n",
    "        temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_pred[i])\n",
    "    return temp/ y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_based(trues, preds):\n",
    "    subset_acc = multilabel_exact_match(preds, trues, num_labels=7).item()\n",
    "    hamming_loss = multilabel_hamming_distance(preds, trues, num_labels=7).item()\n",
    "    acc_exp = Accuracy(preds, trues)\n",
    "    prec_exp = Precision(preds, trues)\n",
    "    rec_exp = Recall(preds, trues)\n",
    "    f1_exp = multilabel_fbeta_score(preds, trues, beta=1.0, num_labels=7).item()\n",
    "    one_error = OneError(preds, trues)\n",
    "    cov = multilabel_coverage_error(preds.float(), trues, num_labels=7).item()\n",
    "    rank_loss = multilabel_ranking_loss(preds.float(), trues, num_labels=7).item()\n",
    "    avg_prec = multilabel_average_precision(preds.float(), trues.long(), num_labels=7).item()\n",
    "\n",
    "    data = {\n",
    "        \"Subset Accuracy\": subset_acc,\n",
    "        \"Hamming Loss\": hamming_loss,\n",
    "        \"Accuracy (Exact Match)\": acc_exp,\n",
    "        \"Precision\": prec_exp,\n",
    "        \"Recall\": rec_exp,\n",
    "        \"F1 Score\": f1_exp,\n",
    "        \"One Error\": one_error,\n",
    "        \"Coverage\": cov,\n",
    "        \"Ranking Loss\": rank_loss,\n",
    "        \"Average Precision\": avg_prec\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([data])  # Creating a DataFrame with a single row\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLC Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clr_exp = []\n",
    "clr_label = []\n",
    "lp_exp = []\n",
    "lp_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_exp = []\n",
    "br_label = []\n",
    "cc_exp = []\n",
    "cc_label = []\n",
    "\n",
    "aa_exp = []\n",
    "aa_label = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0162 - accuracy: 0.4148\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0159 - accuracy: 0.3828\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0159 - accuracy: 0.3814\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0159 - accuracy: 0.3773\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0159 - accuracy: 0.3759\n",
      "3399/3399 [==============================] - 3s 854us/step\n",
      "Train Time: 228.6789526939392 | Test Time: 4.013370513916016\n"
     ]
    }
   ],
   "source": [
    "classifier = DropoutModel(26, 7)\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction)\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "aa_exp.append(example_based(trues, preds))\n",
    "aa_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0162 - accuracy: 0.4522\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0158 - accuracy: 0.4138\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0158 - accuracy: 0.3907\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0158 - accuracy: 0.3985\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0158 - accuracy: 0.3935\n",
      "3399/3399 [==============================] - 3s 980us/step\n",
      "Train Time: 231.09993433952332 | Test Time: 3.9427218437194824\n"
     ]
    }
   ],
   "source": [
    "classifier = ProposedModel1(26, 7)\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction)\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "aa_exp.append(example_based(trues, preds))\n",
    "aa_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0158 - accuracy: 0.3387\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0154 - accuracy: 0.3441\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0154 - accuracy: 0.3414\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0154 - accuracy: 0.3451\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0154 - accuracy: 0.3393\n",
      "3399/3399 [==============================] - 3s 942us/step\n",
      "Train Time: 231.52065515518188 | Test Time: 3.8228495121002197\n"
     ]
    }
   ],
   "source": [
    "classifier = ProposedModel2(26, 7)\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction)\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "aa_exp.append(example_based(trues, preds))\n",
    "aa_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 3ms/step - loss: 0.0162 - accuracy: 0.4724\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0158 - accuracy: 0.4048\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0158 - accuracy: 0.3972\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0158 - accuracy: 0.3935\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0158 - accuracy: 0.3833\n",
      "3399/3399 [==============================] - 3s 879us/step\n",
      "Train Time: 231.20846700668335 | Test Time: 4.1585657596588135\n"
     ]
    }
   ],
   "source": [
    "classifier = WeightedDropoutModel(26, 7)\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train, epochs=5, batch_size=32, verbose=1)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction)\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "aa_exp.append(example_based(trues, preds))\n",
    "aa_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_name = 'aa2'\n",
    "with open(f'Results/{model_name}_exp.pkl', 'wb') as file:\n",
    "    pickle.dump(aa_exp, file)\n",
    "with open(f'Results/{model_name}_label.pkl', 'wb') as file:\n",
    "    pickle.dump(aa_label, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0095 - accuracy: 0.9514\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0087 - accuracy: 0.9516\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0089 - accuracy: 0.9516\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0106 - accuracy: 0.9347\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0106 - accuracy: 0.9348\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0116 - accuracy: 0.9348\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0116 - accuracy: 0.9348\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0108 - accuracy: 0.9348\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0102 - accuracy: 0.9391\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0098 - accuracy: 0.9394\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0099 - accuracy: 0.9394\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0098 - accuracy: 0.9394\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0099 - accuracy: 0.9394\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0097 - accuracy: 0.9487\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0088 - accuracy: 0.9491\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0089 - accuracy: 0.9491\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0089 - accuracy: 0.9490\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0090 - accuracy: 0.9491\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0150 - accuracy: 0.8992\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0147 - accuracy: 0.8993\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0148 - accuracy: 0.8993\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0150 - accuracy: 0.8993\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0147 - accuracy: 0.8993\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0276 - accuracy: 0.6938\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0273 - accuracy: 0.6957\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0273 - accuracy: 0.6953\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0272 - accuracy: 0.6946\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0273 - accuracy: 0.6958\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0299 - accuracy: 0.6537\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0297 - accuracy: 0.6569\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0297 - accuracy: 0.6564\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0296 - accuracy: 0.6582\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0296 - accuracy: 0.6576\n",
      "3399/3399 [==============================] - 3s 887us/step\n",
      "3399/3399 [==============================] - 3s 905us/step\n",
      "3399/3399 [==============================] - 3s 948us/step\n",
      "3399/3399 [==============================] - 3s 911us/step\n",
      "3399/3399 [==============================] - 3s 925us/step\n",
      "3399/3399 [==============================] - 3s 959us/step\n",
      "3399/3399 [==============================] - 3s 920us/step\n",
      "Train Time: 1640.7479932308197 | Test Time: 26.217188596725464\n"
     ]
    }
   ],
   "source": [
    "KERAS_PARAMS = dict(epochs=5, batch_size=32, verbose=1)\n",
    "classifier = BinaryRelevance(classifier=Keras(DropoutModel, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction.toarray())\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "br_exp.append(example_based(trues, preds))\n",
    "br_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0090 - accuracy: 0.9515\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0089 - accuracy: 0.9516\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0089 - accuracy: 0.9516\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0089 - accuracy: 0.9516\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0107 - accuracy: 0.9344\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0103 - accuracy: 0.9348\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0102 - accuracy: 0.9348\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 3ms/step - loss: 0.0102 - accuracy: 0.9348\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0101 - accuracy: 0.9348\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0102 - accuracy: 0.9390\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0098 - accuracy: 0.9394\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0097 - accuracy: 0.9394\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0097 - accuracy: 0.9394\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0097 - accuracy: 0.9394\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 48s 3ms/step - loss: 0.0092 - accuracy: 0.9490\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 3ms/step - loss: 0.0090 - accuracy: 0.9491\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0089 - accuracy: 0.9491\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0089 - accuracy: 0.9491\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0088 - accuracy: 0.9491\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0149 - accuracy: 0.8993\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0147 - accuracy: 0.8993\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0147 - accuracy: 0.8993\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0148 - accuracy: 0.8993\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0148 - accuracy: 0.8993\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0275 - accuracy: 0.6973\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0272 - accuracy: 0.6964\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0270 - accuracy: 0.6940\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0269 - accuracy: 0.6937\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0269 - accuracy: 0.6941\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 48s 3ms/step - loss: 0.0298 - accuracy: 0.6584\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0295 - accuracy: 0.6796\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0294 - accuracy: 0.6812\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 3ms/step - loss: 0.0294 - accuracy: 0.6823\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0294 - accuracy: 0.6812\n",
      "3399/3399 [==============================] - 3s 941us/step\n",
      "3399/3399 [==============================] - 3s 998us/step\n",
      "3399/3399 [==============================] - 3s 982us/step\n",
      "3399/3399 [==============================] - 3s 998us/step\n",
      "3399/3399 [==============================] - 3s 993us/step\n",
      "3399/3399 [==============================] - 3s 1ms/step\n",
      "3399/3399 [==============================] - 3s 998us/step\n",
      "Train Time: 1678.5689840316772 | Test Time: 27.759603261947632\n"
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=10, batch_size=32, verbose=0)\n",
    "classifier = BinaryRelevance(classifier=Keras(ProposedModel1, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction.toarray())\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "br_exp.append(example_based(trues, preds))\n",
    "br_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0086 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0086 - accuracy: 0.9516\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0086 - accuracy: 0.9516\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0087 - accuracy: 0.9516\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0102 - accuracy: 0.9348\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0099 - accuracy: 0.9348\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0100 - accuracy: 0.9348\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 3ms/step - loss: 0.0100 - accuracy: 0.9348\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0100 - accuracy: 0.9348\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0100 - accuracy: 0.9394\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0097 - accuracy: 0.9394\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0095 - accuracy: 0.9394\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0096 - accuracy: 0.9394\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0097 - accuracy: 0.9394\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0089 - accuracy: 0.9489\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0086 - accuracy: 0.9491\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0152 - accuracy: 0.8993\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0149 - accuracy: 0.8993\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0148 - accuracy: 0.8993\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0147 - accuracy: 0.8993\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0147 - accuracy: 0.8993\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0266 - accuracy: 0.7151\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0262 - accuracy: 0.7242\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0262 - accuracy: 0.7245\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0263 - accuracy: 0.7244\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0262 - accuracy: 0.7243\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0293 - accuracy: 0.6634\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0291 - accuracy: 0.6688\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0291 - accuracy: 0.6695\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0291 - accuracy: 0.6704\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0291 - accuracy: 0.6711\n",
      "3399/3399 [==============================] - 3s 941us/step\n",
      "3399/3399 [==============================] - 3s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 3s 990us/step\n",
      "3399/3399 [==============================] - 3s 998us/step\n",
      "3399/3399 [==============================] - 3s 1ms/step\n",
      "3399/3399 [==============================] - 3s 990us/step\n",
      "Train Time: 1684.635294675827 | Test Time: 27.936789751052856\n"
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=10, batch_size=32, verbose=0)\n",
    "classifier = BinaryRelevance(classifier=Keras(ProposedModel2, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction.toarray())\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "br_exp.append(example_based(trues, preds))\n",
    "br_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 48s 3ms/step - loss: 0.0089 - accuracy: 0.9515\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0087 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0087 - accuracy: 0.9516\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0086 - accuracy: 0.9516\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 44s 3ms/step - loss: 0.0087 - accuracy: 0.9516\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0105 - accuracy: 0.9348\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 44s 3ms/step - loss: 0.0103 - accuracy: 0.9348\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 44s 3ms/step - loss: 0.0103 - accuracy: 0.9348\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0103 - accuracy: 0.9348\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 42s 3ms/step - loss: 0.0101 - accuracy: 0.9348\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 42s 3ms/step - loss: 0.0101 - accuracy: 0.9392\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 44s 3ms/step - loss: 0.0097 - accuracy: 0.9394\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 44s 3ms/step - loss: 0.0097 - accuracy: 0.9394\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 43s 3ms/step - loss: 0.0097 - accuracy: 0.9394\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 43s 3ms/step - loss: 0.0097 - accuracy: 0.9394\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 44s 3ms/step - loss: 0.0093 - accuracy: 0.9490\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 44s 3ms/step - loss: 0.0088 - accuracy: 0.9491\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0088 - accuracy: 0.9491\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0089 - accuracy: 0.9491\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0089 - accuracy: 0.9491\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0149 - accuracy: 0.8993\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0146 - accuracy: 0.8993\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0147 - accuracy: 0.8993\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0147 - accuracy: 0.8993\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0151 - accuracy: 0.8993\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0275 - accuracy: 0.6962\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0270 - accuracy: 0.6961\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0270 - accuracy: 0.6945\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0270 - accuracy: 0.6925\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0270 - accuracy: 0.6925\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0299 - accuracy: 0.6532\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0297 - accuracy: 0.6547\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0296 - accuracy: 0.6554\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0296 - accuracy: 0.6552\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0296 - accuracy: 0.6551\n",
      "3399/3399 [==============================] - 3s 859us/step\n",
      "3399/3399 [==============================] - 3s 915us/step\n",
      "3399/3399 [==============================] - 3s 898us/step\n",
      "3399/3399 [==============================] - 3s 919us/step\n",
      "3399/3399 [==============================] - 3s 901us/step\n",
      "3399/3399 [==============================] - 3s 902us/step\n",
      "3399/3399 [==============================] - 3s 926us/step\n",
      "Train Time: 1567.8654870986938 | Test Time: 25.717869758605957\n"
     ]
    }
   ],
   "source": [
    "KERAS_PARAMS = dict(epochs=5, batch_size=32, verbose=1)\n",
    "classifier = BinaryRelevance(classifier=Keras(WeightedDropoutModel, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction.toarray())\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "br_exp.append(example_based(trues, preds))\n",
    "br_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_name = 'br2'\n",
    "with open(f'Results/{model_name}_exp.pkl', 'wb') as file:\n",
    "    pickle.dump(br_exp, file)\n",
    "with open(f'Results/{model_name}_label.pkl', 'wb') as file:\n",
    "    pickle.dump(br_label, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "class ClassifierChain:\n",
    "   \n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def fit(self, X, y, order=None):\n",
    "        \"\"\"Fit classifier with training data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        order: list\n",
    "               The order of the labels to join training\n",
    "        \"\"\"\n",
    "        self.m = X.shape[0]\n",
    "        self.label_count = y.shape[1]\n",
    "        if order is None:\n",
    "            self.order = list(range(y.shape[1]))\n",
    "        elif sorted(order) != list(range(y.shape[1])):\n",
    "            raise ValueError('invalid order of labels')\n",
    "        else:\n",
    "            self.order = order\n",
    "        self.classifiers = []\n",
    "        X_extended = copy.deepcopy(X)\n",
    "        for i in range(self.label_count):\n",
    "            classifier = copy.deepcopy(self.classifier)\n",
    "            y_subset = y[:, self.order[i]]\n",
    "            classifier.fit(X_extended, y_subset)\n",
    "            self.classifiers.append(classifier)\n",
    "            X_extended = np.hstack([X_extended, np.vstack(y_subset)])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_pre):\n",
    "        res = np.zeros((X_pre.shape[0], self.label_count))\n",
    "        for i in range(self.label_count):\n",
    "            y_subset = self.classifiers[i].predict(X_pre)\n",
    "            X_pre = np.hstack([X_pre, np.vstack(y_subset)])\n",
    "            res[:, self.order[i]] = y_subset.flatten()\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0089 - accuracy: 0.9516\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0087 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0119 - accuracy: 0.9341\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0112 - accuracy: 0.9348\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0102 - accuracy: 0.9349\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0103 - accuracy: 0.9348\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0103 - accuracy: 0.9345\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 48s 3ms/step - loss: 0.0091 - accuracy: 0.9396\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0087 - accuracy: 0.9414\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0087 - accuracy: 0.9418\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0088 - accuracy: 0.9417\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0088 - accuracy: 0.9425\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0089 - accuracy: 0.9491\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0148 - accuracy: 0.8993\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0146 - accuracy: 0.8993\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 3ms/step - loss: 0.0149 - accuracy: 0.8993\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0150 - accuracy: 0.8993\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0147 - accuracy: 0.8993\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 48s 3ms/step - loss: 0.0274 - accuracy: 0.6908\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0272 - accuracy: 0.6932\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0271 - accuracy: 0.6941\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0272 - accuracy: 0.6918\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0272 - accuracy: 0.6917\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0298 - accuracy: 0.6570\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0297 - accuracy: 0.6571\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0295 - accuracy: 0.6602\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0295 - accuracy: 0.6593\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0296 - accuracy: 0.6587\n",
      "3399/3399 [==============================] - 3s 913us/step\n",
      "3399/3399 [==============================] - 3s 962us/step\n",
      "3399/3399 [==============================] - 3s 973us/step\n",
      "3399/3399 [==============================] - 3s 975us/step\n",
      "3399/3399 [==============================] - 3s 962us/step\n",
      "3399/3399 [==============================] - 3s 949us/step\n",
      "3399/3399 [==============================] - 3s 933us/step\n",
      "Train Time: 1651.4836361408234 | Test Time: 27.50615668296814\n"
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=10, batch_size=32, verbose=0)\n",
    "classifier = ClassifierChain(classifier=Keras(DropoutModel, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction)\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "cc_exp.append(example_based(trues, preds))\n",
    "cc_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0090 - accuracy: 0.9514\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0087 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0087 - accuracy: 0.9516\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0104 - accuracy: 0.9347\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0102 - accuracy: 0.9348\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0102 - accuracy: 0.9349\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0101 - accuracy: 0.9349\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0101 - accuracy: 0.9348\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0091 - accuracy: 0.9418\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0088 - accuracy: 0.9424\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0088 - accuracy: 0.9427\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0088 - accuracy: 0.9427\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0089 - accuracy: 0.9428\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0090 - accuracy: 0.9491\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0086 - accuracy: 0.9491\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0149 - accuracy: 0.8993\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0148 - accuracy: 0.8993\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0148 - accuracy: 0.8993\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0148 - accuracy: 0.8993\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0147 - accuracy: 0.8993\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0275 - accuracy: 0.6979\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0272 - accuracy: 0.6965\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0270 - accuracy: 0.6932\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0269 - accuracy: 0.6941\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0269 - accuracy: 0.6947\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0299 - accuracy: 0.6567\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0297 - accuracy: 0.6589\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0295 - accuracy: 0.6741\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0294 - accuracy: 0.6826\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0294 - accuracy: 0.6823\n",
      "3399/3399 [==============================] - 3s 943us/step\n",
      "3399/3399 [==============================] - 3s 965us/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 3s 954us/step\n",
      "3399/3399 [==============================] - 3s 985us/step\n",
      "3399/3399 [==============================] - 3s 964us/step\n",
      "3399/3399 [==============================] - 3s 974us/step\n",
      "Train Time: 1706.1791026592255 | Test Time: 28.126394987106323\n"
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=10, batch_size=32, verbose=0)\n",
    "classifier = ClassifierChain(classifier=Keras(ProposedModel1, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction)\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "cc_exp.append(example_based(trues, preds))\n",
    "cc_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0085 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0085 - accuracy: 0.9516\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0086 - accuracy: 0.9516\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0086 - accuracy: 0.9516\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0103 - accuracy: 0.9346\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0103 - accuracy: 0.9348\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0106 - accuracy: 0.9348\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0102 - accuracy: 0.9348\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0100 - accuracy: 0.9348\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0087 - accuracy: 0.9421\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0083 - accuracy: 0.9434\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0083 - accuracy: 0.9433\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0083 - accuracy: 0.9434\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0083 - accuracy: 0.9436\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0088 - accuracy: 0.9490\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0086 - accuracy: 0.9491\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0086 - accuracy: 0.9491\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0148 - accuracy: 0.8991\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0149 - accuracy: 0.8993\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0150 - accuracy: 0.8993\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0148 - accuracy: 0.8993\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0147 - accuracy: 0.8993\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0272 - accuracy: 0.7096\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0264 - accuracy: 0.7097\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0262 - accuracy: 0.7111\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0262 - accuracy: 0.7119\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0261 - accuracy: 0.7123\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0291 - accuracy: 0.6817\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0288 - accuracy: 0.6855\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0288 - accuracy: 0.6862\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0288 - accuracy: 0.6867\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0288 - accuracy: 0.6864\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "Train Time: 1727.6977133750916 | Test Time: 30.724639892578125\n"
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=10, batch_size=32, verbose=0)\n",
    "classifier = ClassifierChain(classifier=Keras(ProposedModel2, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction)\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "cc_exp.append(example_based(trues, preds))\n",
    "cc_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0090 - accuracy: 0.9512\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0086 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0086 - accuracy: 0.9516\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0086 - accuracy: 0.9516\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0086 - accuracy: 0.9516\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0104 - accuracy: 0.9347\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0102 - accuracy: 0.9348\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0101 - accuracy: 0.9348\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0101 - accuracy: 0.9348\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0101 - accuracy: 0.9348\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0090 - accuracy: 0.9417\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0088 - accuracy: 0.9427\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0088 - accuracy: 0.9429\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0088 - accuracy: 0.9426\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0089 - accuracy: 0.9425\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0089 - accuracy: 0.9490\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0086 - accuracy: 0.9491\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0086 - accuracy: 0.9491\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0086 - accuracy: 0.9491\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0086 - accuracy: 0.9491\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0149 - accuracy: 0.8990\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0146 - accuracy: 0.8993\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0146 - accuracy: 0.8993\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0146 - accuracy: 0.8993\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0146 - accuracy: 0.8993\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0274 - accuracy: 0.6962\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0270 - accuracy: 0.6939\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0269 - accuracy: 0.6939\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0269 - accuracy: 0.6927\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0270 - accuracy: 0.6921\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 46s 3ms/step - loss: 0.0298 - accuracy: 0.6530\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0296 - accuracy: 0.6539\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0296 - accuracy: 0.6549\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0296 - accuracy: 0.6559\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 45s 3ms/step - loss: 0.0295 - accuracy: 0.6561\n",
      "3399/3399 [==============================] - 3s 859us/step\n",
      "3399/3399 [==============================] - 3s 903us/step\n",
      "3399/3399 [==============================] - 3s 936us/step\n",
      "3399/3399 [==============================] - 3s 939us/step\n",
      "3399/3399 [==============================] - 3s 914us/step\n",
      "3399/3399 [==============================] - 3s 912us/step\n",
      "3399/3399 [==============================] - 3s 947us/step\n",
      "Train Time: 1588.1042520999908 | Test Time: 26.637101888656616\n"
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=10, batch_size=32, verbose=0)\n",
    "classifier = ClassifierChain(classifier=Keras(WeightedDropoutModel, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction)\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "cc_exp.append(example_based(trues, preds))\n",
    "cc_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_name = 'cc2'\n",
    "with open(f'Results/{model_name}_exp.pkl', 'wb') as file:\n",
    "    pickle.dump(cc_exp, file)\n",
    "with open(f'Results/{model_name}_label.pkl', 'wb') as file:\n",
    "    pickle.dump(cc_label, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  0, ...,  3, 12, 34])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp = LabelPowerset()\n",
    "y_temp = lp.transform(y_test)\n",
    "y_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 36988]\n",
      " [    1 14100]\n",
      " [    2   105]\n",
      " [    3 13100]\n",
      " [    4 19812]\n",
      " [    5  2121]\n",
      " [    6   243]\n",
      " [    7    96]\n",
      " [    8   496]\n",
      " [    9  1941]\n",
      " [   10  1393]\n",
      " [   11   336]\n",
      " [   12  2300]\n",
      " [   13   181]\n",
      " [   14   338]\n",
      " [   15   110]\n",
      " [   16   588]\n",
      " [   17   475]\n",
      " [   18    80]\n",
      " [   19    79]\n",
      " [   20   141]\n",
      " [   21   875]\n",
      " [   22   529]\n",
      " [   23   285]\n",
      " [   24   623]\n",
      " [   25   190]\n",
      " [   26   440]\n",
      " [   27   508]\n",
      " [   28   342]\n",
      " [   29   957]\n",
      " [   30   618]\n",
      " [   31   465]\n",
      " [   32   552]\n",
      " [   33   179]\n",
      " [   34   832]\n",
      " [   35   643]\n",
      " [   36   124]\n",
      " [   37    10]\n",
      " [   38    99]\n",
      " [   39   621]\n",
      " [   40   151]\n",
      " [   41   146]\n",
      " [   42    92]\n",
      " [   43    61]\n",
      " [   44   565]\n",
      " [   45   268]\n",
      " [   46   154]\n",
      " [   47   107]\n",
      " [   48    47]\n",
      " [   49    34]\n",
      " [   50    88]\n",
      " [   51    14]\n",
      " [   52    75]\n",
      " [   53    96]\n",
      " [   54    45]\n",
      " [   55    74]\n",
      " [   56    10]\n",
      " [   57    83]\n",
      " [   58    16]\n",
      " [   59    31]\n",
      " [   60   145]\n",
      " [   61    12]\n",
      " [   62    57]\n",
      " [   63   187]\n",
      " [   64    88]\n",
      " [   65   130]\n",
      " [   66   115]\n",
      " [   67    18]\n",
      " [   68    42]\n",
      " [   69    13]\n",
      " [   70     8]\n",
      " [   71    74]\n",
      " [   72   104]\n",
      " [   73    78]\n",
      " [   74    36]\n",
      " [   75    76]\n",
      " [   76   133]\n",
      " [   77   130]\n",
      " [   78    37]\n",
      " [   79     3]\n",
      " [   80    52]\n",
      " [   81    47]\n",
      " [   82    35]\n",
      " [   83    27]\n",
      " [   84    41]\n",
      " [   85    44]\n",
      " [   86    48]\n",
      " [   87    63]\n",
      " [   88    36]\n",
      " [   89    16]\n",
      " [   90    23]\n",
      " [   91    22]\n",
      " [   92    35]\n",
      " [   93    53]\n",
      " [   94    21]\n",
      " [   95     9]\n",
      " [   96    20]\n",
      " [   97    37]\n",
      " [   98    81]\n",
      " [   99    36]\n",
      " [  100     4]\n",
      " [  101    18]\n",
      " [  102    45]\n",
      " [  103    15]\n",
      " [  104    44]\n",
      " [  105    14]\n",
      " [  106    10]\n",
      " [  107     8]\n",
      " [  108    29]\n",
      " [  109    38]\n",
      " [  110    31]\n",
      " [  111    13]\n",
      " [  112    16]\n",
      " [  113    16]\n",
      " [  114     5]\n",
      " [  115    19]\n",
      " [  116    17]\n",
      " [  117     3]\n",
      " [  118    11]\n",
      " [  119    35]\n",
      " [  120    11]\n",
      " [  121    11]\n",
      " [  122     5]\n",
      " [  123    11]\n",
      " [  124     8]\n",
      " [  125     6]\n",
      " [  126     1]\n",
      " [  127     7]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_temp, return_counts=True)\n",
    "\n",
    "#display unique values and counts side by side\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435020"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108755"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_exp = []\n",
    "lp_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LPDropoutModel(n_inputs=26, n_outputs=7):\n",
    "    model = DNNDropout(num_features=n_inputs, num_classes=n_outputs, p=0.3)\n",
    "    lossWeights = weight_list\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13595/13595 [==============================] - 40s 3ms/step - loss: -81.5739 - accuracy: 0.1804\n",
      "3399/3399 [==============================] - 3s 952us/step\n",
      "Train Time: 41.539960861206055 | Test Time: 7.273635387420654\n",
      "   Subset Accuracy  Hamming Loss Accuracy (Exact Match)  \\\n",
      "0         0.182171       0.18329         tensor(0.8075)   \n",
      "\n",
      "                             Precision          Recall  F1 Score  One Error  \\\n",
      "0  tensor(0.9084, dtype=torch.float64)  tensor(0.8814)  0.798786   0.071991   \n",
      "\n",
      "   Coverage  Ranking Loss  Average Precision  \n",
      "0  6.568958      0.315454            0.83698  \n"
     ]
    }
   ],
   "source": [
    "KERAS_PARAMS = dict(epochs=1, batch_size=32, verbose=1)\n",
    "classifier1 = LabelPowerset(classifier=Keras(LPDropoutModel, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier1.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier1.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction.toarray())\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "lp_exp.append(example_based(trues, preds))\n",
    "lp_label.append(label_based(trues, preds))\n",
    "print(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LPWeightedDropoutModel(n_inputs=26, n_outputs=7):\n",
    "    model = DNNWeightedDropout(num_features=n_inputs, num_classes=n_outputs, p=0.3)\n",
    "    lossWeights = weight_list\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13595/13595 [==============================] - 44s 3ms/step - loss: -79.9135 - accuracy: 0.1806\n",
      "3399/3399 [==============================] - 3s 978us/step\n",
      "Train Time: 45.3110888004303 | Test Time: 7.460609674453735\n",
      "   Subset Accuracy  Hamming Loss Accuracy (Exact Match)  \\\n",
      "0         0.182171       0.18329         tensor(0.8075)   \n",
      "\n",
      "                             Precision          Recall  F1 Score  One Error  \\\n",
      "0  tensor(0.9084, dtype=torch.float64)  tensor(0.8814)  0.798786   0.071991   \n",
      "\n",
      "   Coverage  Ranking Loss  Average Precision  \n",
      "0  6.568958      0.315454            0.83698  \n"
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=10, batch_size=32, verbose=0)\n",
    "classifier2 = LabelPowerset(classifier=Keras(LPProposedModel1, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier2.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier2.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction.toarray())\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "lp_exp.append(example_based(trues, preds))\n",
    "lp_label.append(label_based(trues, preds))\n",
    "print(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13595/13595 [==============================] - 44s 3ms/step - loss: -77.9858 - accuracy: 0.1806\n",
      "3399/3399 [==============================] - 3s 1ms/step\n",
      "Train Time: 45.76464009284973 | Test Time: 7.314046382904053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m lp_exp\u001b[38;5;241m.\u001b[39mappend(example_based(trues, preds))\n\u001b[0;32m     17\u001b[0m lp_label\u001b[38;5;241m.\u001b[39mappend(label_based(trues, preds))\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlabel_based\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[51], line 8\u001b[0m, in \u001b[0;36mlabel_based\u001b[1;34m(trues, preds)\u001b[0m\n\u001b[0;32m      6\u001b[0m rec_exp \u001b[38;5;241m=\u001b[39m Recall(preds, trues)\n\u001b[0;32m      7\u001b[0m f1_exp \u001b[38;5;241m=\u001b[39m multilabel_fbeta_score(preds, trues, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m----> 8\u001b[0m one_error \u001b[38;5;241m=\u001b[39m \u001b[43mOneError\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m cov \u001b[38;5;241m=\u001b[39m multilabel_coverage_error(preds\u001b[38;5;241m.\u001b[39mfloat(), trues, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     10\u001b[0m rank_loss \u001b[38;5;241m=\u001b[39m multilabel_ranking_loss(preds\u001b[38;5;241m.\u001b[39mfloat(), trues, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[1;32mIn[50], line 25\u001b[0m, in \u001b[0;36mOneError\u001b[1;34m(outputs, test_target)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(test_data_num):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(test_target[i]) \u001b[38;5;241m!=\u001b[39m class_num \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28msum\u001b[39m(test_target[i]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 25\u001b[0m         Max, index \u001b[38;5;241m=\u001b[39m \u001b[43mfindmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m         num \u001b[38;5;241m=\u001b[39m num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m test_target[i][index] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[50], line 12\u001b[0m, in \u001b[0;36mfindmax\u001b[1;34m(outputs)\u001b[0m\n\u001b[0;32m      9\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(outputs)):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m Max:\n\u001b[0;32m     13\u001b[0m         Max \u001b[38;5;241m=\u001b[39m outputs[i]\n\u001b[0;32m     14\u001b[0m         index \u001b[38;5;241m=\u001b[39m i\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=10, batch_size=32, verbose=0)\n",
    "classifier = LabelPowerset(classifier=Keras(LPProposedModel2, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction.toarray())\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "lp_exp.append(example_based(trues, preds))\n",
    "lp_label.append(label_based(trues, preds))\n",
    "print(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3399/3399 [==============================] - 3s 880us/step\n",
      "Train Time: 205.77587723731995 | Test Time: 7.054391145706177\n",
      "   Subset Accuracy  Hamming Loss Accuracy (Exact Match)  \\\n",
      "0         0.004257      0.444514         tensor(0.5231)   \n",
      "\n",
      "                             Precision          Recall  F1 Score  One Error  \\\n",
      "0  tensor(0.5804, dtype=torch.float64)  tensor(0.8431)  0.517714   0.071952   \n",
      "\n",
      "   Coverage  Ranking Loss  Average Precision  \n",
      "0  6.933373      0.440487           0.836675  \n"
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=5, batch_size=32, verbose=0)\n",
    "classifier = LabelPowerset(classifier=Keras(LPWeightedDropoutModel, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction.toarray())\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "lp_exp.append(example_based(trues, preds))\n",
    "lp_label.append(label_based(trues, preds))\n",
    "print(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_name = 'lp2'\n",
    "with open(f'Results/{model_name}_exp.pkl', 'wb') as file:\n",
    "    pickle.dump(lp_exp, file)\n",
    "with open(f'Results/{model_name}_label.pkl', 'wb') as file:\n",
    "    pickle.dump(lp_label, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalibratedLabelRanking:\n",
    "    \"\"\"Calibratedn Label Ranking\n",
    "    Reference Paper:\n",
    "        Min-Ling Zhang and Zhi-Hua Zhou. A Review on Multi-Label Learning Algorithms\n",
    "        Johannes Fürnkranz. Multilabel classification via calibrated label ranking\n",
    "    \"\"\"\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.virtualLabel = y.shape[1]\n",
    "        self.label_count = y.shape[1]\n",
    "        self.m = X.shape[0]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.realLabelClassifiers = []\n",
    "        self.virtualLabelClassifiers = []\n",
    "        self.trainRealLabelClassifiers()\n",
    "        self.trainVirtualLabelClassifiers()\n",
    "        return self\n",
    "\n",
    "    def trainRealLabelClassifiers(self):\n",
    "        for i in range(self.label_count):\n",
    "            for j in range(i+1, self.label_count):\n",
    "                X_subset = []\n",
    "                y_subset = []\n",
    "                classifier = copy.deepcopy(self.classifier)\n",
    "                for k in range(self.m):\n",
    "                    if self.y[k, i] == self.y[k, j]:\n",
    "                        continue\n",
    "                    elif self.y[k, i] == 1:\n",
    "                        X_subset.append(self.X[k, :])\n",
    "                        y_subset.append(i)\n",
    "                    else:\n",
    "                        X_subset.append(self.X[k, :])\n",
    "                        y_subset.append(j)\n",
    "                try:\n",
    "                    classifier.fit(np.vstack(X_subset), np.array(y_subset))\n",
    "                except ValueError:\n",
    "                    if y_subset[0] == i:\n",
    "                        classifier = i\n",
    "                    else:\n",
    "                        classifier = j\n",
    "                self.realLabelClassifiers.append(classifier)\n",
    "\n",
    "    def trainVirtualLabelClassifiers(self):\n",
    "        for i in range(self.label_count):\n",
    "            classifier = copy.deepcopy(self.classifier)\n",
    "            y_temp= self.y[:, i]\n",
    "            y_subset = []\n",
    "            for j in range(self.m):\n",
    "                y_subset.append(i if y_temp[j] == 1 else self.virtualLabel)\n",
    "            classifier.fit(self.X, y_subset)\n",
    "            self.virtualLabelClassifiers.append(classifier)\n",
    "\n",
    "    def predict(self, X_pre):\n",
    "        result = np.zeros((X_pre.shape[0], self.label_count))\n",
    "        threshold = self.voteForVirtualLabel(X_pre)\n",
    "        votes = self.voteForRealLabel(X_pre)\n",
    "        for i in range(X_pre.shape[0]):\n",
    "            result[i, votes[i,:] > threshold[i]] = 1\n",
    "        return result\n",
    "\n",
    "    def voteForVirtualLabel(self, X_pre):\n",
    "        votes = np.zeros(X_pre.shape[0])\n",
    "        for i in range(len(self.virtualLabelClassifiers)):\n",
    "            predict = self.virtualLabelClassifiers[i].predict(X_pre)\n",
    "            for j in range(X_pre.shape[0]):\n",
    "                if predict[j] == self.virtualLabel:\n",
    "                    votes[j] += 1\n",
    "        return votes\n",
    "\n",
    "    def voteForRealLabel(self, X_pre):\n",
    "        votes = np.zeros((X_pre.shape[0], self.label_count))\n",
    "        for i in range(len(self.realLabelClassifiers)):\n",
    "            if isinstance(self.realLabelClassifiers[i], int):\n",
    "                predict = np.array([self.realLabelClassifiers[i]] * X_pre.shape[0])\n",
    "            else:\n",
    "                predict = self.realLabelClassifiers[i].predict(X_pre)\n",
    "            for j in range(X_pre.shape[0]):\n",
    "                votes[j, predict[j]] += 1\n",
    "        for j in range(len(self.virtualLabelClassifiers)):\n",
    "            predict = self.virtualLabelClassifiers[j].predict(X_pre)\n",
    "            for j in range(X_pre.shape[0]):\n",
    "                if predict[j] < self.virtualLabel:\n",
    "                    votes[j, predict[j]] += 1\n",
    "        return votes\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1255/1255 [==============================] - 5s 4ms/step - loss: 0.0320 - accuracy: 0.6052\n",
      "Epoch 2/5\n",
      "1255/1255 [==============================] - 4s 3ms/step - loss: 0.0317 - accuracy: 0.6127\n",
      "Epoch 3/5\n",
      "1255/1255 [==============================] - 4s 3ms/step - loss: 0.0316 - accuracy: 0.6156\n",
      "Epoch 4/5\n",
      "1255/1255 [==============================] - 4s 3ms/step - loss: 0.0316 - accuracy: 0.6157\n",
      "Epoch 5/5\n",
      "1255/1255 [==============================] - 4s 3ms/step - loss: 0.0315 - accuracy: 0.6200\n",
      "Epoch 1/5\n",
      "1235/1235 [==============================] - 5s 4ms/step - loss: 0.0321 - accuracy: 0.5927\n",
      "Epoch 2/5\n",
      "1235/1235 [==============================] - 4s 3ms/step - loss: 0.0317 - accuracy: 0.6068\n",
      "Epoch 3/5\n",
      "1235/1235 [==============================] - 4s 4ms/step - loss: 0.0316 - accuracy: 0.6065\n",
      "Epoch 4/5\n",
      "1235/1235 [==============================] - 4s 4ms/step - loss: 0.0315 - accuracy: 0.6114\n",
      "Epoch 5/5\n",
      "1235/1235 [==============================] - 4s 3ms/step - loss: 0.0316 - accuracy: 0.6105\n",
      "Epoch 1/5\n",
      "1157/1157 [==============================] - 5s 4ms/step - loss: 0.0334 - accuracy: 0.5385\n",
      "Epoch 2/5\n",
      "1157/1157 [==============================] - 4s 3ms/step - loss: 0.0329 - accuracy: 0.5688\n",
      "Epoch 3/5\n",
      "1157/1157 [==============================] - 4s 4ms/step - loss: 0.0328 - accuracy: 0.5752\n",
      "Epoch 4/5\n",
      "1157/1157 [==============================] - 4s 4ms/step - loss: 0.0328 - accuracy: 0.5743\n",
      "Epoch 5/5\n",
      "1157/1157 [==============================] - 4s 3ms/step - loss: 0.0328 - accuracy: 0.5746\n",
      "Epoch 1/5\n",
      "1730/1730 [==============================] - 7s 4ms/step - loss: 0.0285 - accuracy: 0.7092\n",
      "Epoch 2/5\n",
      "1730/1730 [==============================] - 6s 3ms/step - loss: 0.0280 - accuracy: 0.7143\n",
      "Epoch 3/5\n",
      "1730/1730 [==============================] - 6s 3ms/step - loss: 0.0280 - accuracy: 0.7157\n",
      "Epoch 4/5\n",
      "1730/1730 [==============================] - 6s 3ms/step - loss: 0.0279 - accuracy: 0.7148\n",
      "Epoch 5/5\n",
      "1730/1730 [==============================] - 6s 3ms/step - loss: 0.0279 - accuracy: 0.7166\n",
      "Epoch 1/5\n",
      "5079/5079 [==============================] - 19s 4ms/step - loss: 0.0093 - accuracy: 0.9550\n",
      "Epoch 2/5\n",
      "5079/5079 [==============================] - 18s 4ms/step - loss: 0.0085 - accuracy: 0.9565\n",
      "Epoch 3/5\n",
      "5079/5079 [==============================] - 19s 4ms/step - loss: 0.0085 - accuracy: 0.9565\n",
      "Epoch 4/5\n",
      "5079/5079 [==============================] - 17s 3ms/step - loss: 0.0085 - accuracy: 0.9565\n",
      "Epoch 5/5\n",
      "5079/5079 [==============================] - 18s 4ms/step - loss: 0.0085 - accuracy: 0.9565\n",
      "Epoch 1/5\n",
      "5741/5741 [==============================] - 20s 3ms/step - loss: 0.0094 - accuracy: 0.9501\n",
      "Epoch 2/5\n",
      "5741/5741 [==============================] - 22s 4ms/step - loss: 0.0088 - accuracy: 0.9509\n",
      "Epoch 3/5\n",
      "5741/5741 [==============================] - 21s 4ms/step - loss: 0.0088 - accuracy: 0.9509\n",
      "Epoch 4/5\n",
      "5741/5741 [==============================] - 20s 4ms/step - loss: 0.0088 - accuracy: 0.9509\n",
      "Epoch 5/5\n",
      "5741/5741 [==============================] - 20s 3ms/step - loss: 0.0088 - accuracy: 0.9509\n",
      "Epoch 1/5\n",
      "864/864 [==============================] - 4s 4ms/step - loss: 0.0331 - accuracy: 0.5481\n",
      "Epoch 2/5\n",
      "864/864 [==============================] - 3s 3ms/step - loss: 0.0329 - accuracy: 0.5722\n",
      "Epoch 3/5\n",
      "864/864 [==============================] - 3s 4ms/step - loss: 0.0328 - accuracy: 0.5741\n",
      "Epoch 4/5\n",
      "864/864 [==============================] - 3s 3ms/step - loss: 0.0328 - accuracy: 0.5754\n",
      "Epoch 5/5\n",
      "864/864 [==============================] - 3s 3ms/step - loss: 0.0328 - accuracy: 0.5763\n",
      "Epoch 1/5\n",
      "1228/1228 [==============================] - 5s 4ms/step - loss: 0.0325 - accuracy: 0.5850\n",
      "Epoch 2/5\n",
      "1228/1228 [==============================] - 4s 4ms/step - loss: 0.0322 - accuracy: 0.5973\n",
      "Epoch 3/5\n",
      "1228/1228 [==============================] - 4s 4ms/step - loss: 0.0321 - accuracy: 0.5951\n",
      "Epoch 4/5\n",
      "1228/1228 [==============================] - 4s 4ms/step - loss: 0.0321 - accuracy: 0.5955\n",
      "Epoch 5/5\n",
      "1228/1228 [==============================] - 4s 4ms/step - loss: 0.0320 - accuracy: 0.5965\n",
      "Epoch 1/5\n",
      "1915/1915 [==============================] - 9s 4ms/step - loss: 0.0301 - accuracy: 0.6501\n",
      "Epoch 2/5\n",
      "1915/1915 [==============================] - 7s 3ms/step - loss: 0.0298 - accuracy: 0.6527\n",
      "Epoch 3/5\n",
      "1915/1915 [==============================] - 7s 3ms/step - loss: 0.0297 - accuracy: 0.6517\n",
      "Epoch 4/5\n",
      "1915/1915 [==============================] - 7s 4ms/step - loss: 0.0297 - accuracy: 0.6550\n",
      "Epoch 5/5\n",
      "1915/1915 [==============================] - 7s 4ms/step - loss: 0.0297 - accuracy: 0.6571\n",
      "Epoch 1/5\n",
      "4962/4962 [==============================] - 18s 3ms/step - loss: 0.0112 - accuracy: 0.9416\n",
      "Epoch 2/5\n",
      "4962/4962 [==============================] - 18s 4ms/step - loss: 0.0104 - accuracy: 0.9442\n",
      "Epoch 3/5\n",
      "4962/4962 [==============================] - 19s 4ms/step - loss: 0.0104 - accuracy: 0.9442\n",
      "Epoch 4/5\n",
      "4962/4962 [==============================] - 17s 3ms/step - loss: 0.0103 - accuracy: 0.9442\n",
      "Epoch 5/5\n",
      "4962/4962 [==============================] - 19s 4ms/step - loss: 0.0100 - accuracy: 0.9442\n",
      "Epoch 1/5\n",
      "5747/5747 [==============================] - 22s 4ms/step - loss: 0.0112 - accuracy: 0.9306\n",
      "Epoch 2/5\n",
      "5747/5747 [==============================] - 20s 4ms/step - loss: 0.0105 - accuracy: 0.9306\n",
      "Epoch 3/5\n",
      "5747/5747 [==============================] - 20s 4ms/step - loss: 0.0105 - accuracy: 0.9306\n",
      "Epoch 4/5\n",
      "5747/5747 [==============================] - 21s 4ms/step - loss: 0.0105 - accuracy: 0.9306\n",
      "Epoch 5/5\n",
      "5747/5747 [==============================] - 23s 4ms/step - loss: 0.0105 - accuracy: 0.9306\n",
      "Epoch 1/5\n",
      "1123/1123 [==============================] - 4s 4ms/step - loss: 0.0326 - accuracy: 0.5757\n",
      "Epoch 2/5\n",
      "1123/1123 [==============================] - 4s 3ms/step - loss: 0.0322 - accuracy: 0.5908\n",
      "Epoch 3/5\n",
      "1123/1123 [==============================] - 4s 4ms/step - loss: 0.0322 - accuracy: 0.5901\n",
      "Epoch 4/5\n",
      "1123/1123 [==============================] - 4s 4ms/step - loss: 0.0322 - accuracy: 0.5906\n",
      "Epoch 5/5\n",
      "1123/1123 [==============================] - 4s 4ms/step - loss: 0.0322 - accuracy: 0.5902\n",
      "Epoch 1/5\n",
      "1886/1886 [==============================] - 8s 4ms/step - loss: 0.0297 - accuracy: 0.6624\n",
      "Epoch 2/5\n",
      "1886/1886 [==============================] - 7s 4ms/step - loss: 0.0289 - accuracy: 0.6766\n",
      "Epoch 3/5\n",
      "1886/1886 [==============================] - 7s 4ms/step - loss: 0.0288 - accuracy: 0.6806\n",
      "Epoch 4/5\n",
      "1886/1886 [==============================] - 7s 3ms/step - loss: 0.0287 - accuracy: 0.6819\n",
      "Epoch 5/5\n",
      "1886/1886 [==============================] - 7s 3ms/step - loss: 0.0287 - accuracy: 0.6840\n",
      "Epoch 1/5\n",
      "5041/5041 [==============================] - 21s 4ms/step - loss: 0.0109 - accuracy: 0.9435\n",
      "Epoch 2/5\n",
      "5041/5041 [==============================] - 18s 4ms/step - loss: 0.0105 - accuracy: 0.9435\n",
      "Epoch 3/5\n",
      "5041/5041 [==============================] - 19s 4ms/step - loss: 0.0105 - accuracy: 0.9435\n",
      "Epoch 4/5\n",
      "5041/5041 [==============================] - 18s 4ms/step - loss: 0.0105 - accuracy: 0.9435\n",
      "Epoch 5/5\n",
      "5041/5041 [==============================] - 17s 3ms/step - loss: 0.0101 - accuracy: 0.9435\n",
      "Epoch 1/5\n",
      "5775/5775 [==============================] - 22s 4ms/step - loss: 0.0110 - accuracy: 0.9327\n",
      "Epoch 2/5\n",
      "5775/5775 [==============================] - 21s 4ms/step - loss: 0.0103 - accuracy: 0.9340\n",
      "Epoch 3/5\n",
      "5775/5775 [==============================] - 21s 4ms/step - loss: 0.0103 - accuracy: 0.9340\n",
      "Epoch 4/5\n",
      "5775/5775 [==============================] - 20s 3ms/step - loss: 0.0104 - accuracy: 0.9340\n",
      "Epoch 5/5\n",
      "5775/5775 [==============================] - 20s 3ms/step - loss: 0.0106 - accuracy: 0.9339\n",
      "Epoch 1/5\n",
      "1799/1799 [==============================] - 7s 4ms/step - loss: 0.0289 - accuracy: 0.6870\n",
      "Epoch 2/5\n",
      "1799/1799 [==============================] - 6s 4ms/step - loss: 0.0285 - accuracy: 0.6886\n",
      "Epoch 3/5\n",
      "1799/1799 [==============================] - 6s 4ms/step - loss: 0.0285 - accuracy: 0.6910\n",
      "Epoch 4/5\n",
      "1799/1799 [==============================] - 6s 4ms/step - loss: 0.0285 - accuracy: 0.6922\n",
      "Epoch 5/5\n",
      "1799/1799 [==============================] - 6s 4ms/step - loss: 0.0284 - accuracy: 0.6959\n",
      "Epoch 1/5\n",
      "5084/5084 [==============================] - 19s 4ms/step - loss: 0.0099 - accuracy: 0.9520\n",
      "Epoch 2/5\n",
      "5084/5084 [==============================] - 18s 4ms/step - loss: 0.0091 - accuracy: 0.9526\n",
      "Epoch 3/5\n",
      "5084/5084 [==============================] - 18s 4ms/step - loss: 0.0091 - accuracy: 0.9526\n",
      "Epoch 4/5\n",
      "5084/5084 [==============================] - 18s 4ms/step - loss: 0.0092 - accuracy: 0.9526\n",
      "Epoch 5/5\n",
      "5084/5084 [==============================] - 18s 4ms/step - loss: 0.0092 - accuracy: 0.9526\n",
      "Epoch 1/5\n",
      "5737/5737 [==============================] - 23s 4ms/step - loss: 0.0097 - accuracy: 0.9478\n",
      "Epoch 2/5\n",
      "5737/5737 [==============================] - 20s 4ms/step - loss: 0.0091 - accuracy: 0.9482\n",
      "Epoch 3/5\n",
      "5737/5737 [==============================] - 20s 4ms/step - loss: 0.0092 - accuracy: 0.9482\n",
      "Epoch 4/5\n",
      "5737/5737 [==============================] - 21s 4ms/step - loss: 0.0092 - accuracy: 0.9482\n",
      "Epoch 5/5\n",
      "5737/5737 [==============================] - 21s 4ms/step - loss: 0.0091 - accuracy: 0.9482\n",
      "Epoch 1/5\n",
      "5078/5078 [==============================] - 21s 4ms/step - loss: 0.0171 - accuracy: 0.8859\n",
      "Epoch 2/5\n",
      "5078/5078 [==============================] - 19s 4ms/step - loss: 0.0168 - accuracy: 0.8865\n",
      "Epoch 3/5\n",
      "5078/5078 [==============================] - 18s 4ms/step - loss: 0.0168 - accuracy: 0.8865\n",
      "Epoch 4/5\n",
      "5078/5078 [==============================] - 18s 4ms/step - loss: 0.0168 - accuracy: 0.8865\n",
      "Epoch 5/5\n",
      "5078/5078 [==============================] - 18s 4ms/step - loss: 0.0169 - accuracy: 0.8865\n",
      "Epoch 1/5\n",
      "5882/5882 [==============================] - 24s 4ms/step - loss: 0.0162 - accuracy: 0.8790\n",
      "Epoch 2/5\n",
      "5882/5882 [==============================] - 22s 4ms/step - loss: 0.0158 - accuracy: 0.8796\n",
      "Epoch 3/5\n",
      "5882/5882 [==============================] - 21s 4ms/step - loss: 0.0158 - accuracy: 0.8796\n",
      "Epoch 4/5\n",
      "5882/5882 [==============================] - 21s 4ms/step - loss: 0.0158 - accuracy: 0.8796\n",
      "Epoch 5/5\n",
      "5882/5882 [==============================] - 21s 4ms/step - loss: 0.0158 - accuracy: 0.8796\n",
      "Epoch 1/5\n",
      "5396/5396 [==============================] - 21s 4ms/step - loss: 0.0281 - accuracy: 0.6826\n",
      "Epoch 2/5\n",
      "5396/5396 [==============================] - 20s 4ms/step - loss: 0.0277 - accuracy: 0.6993\n",
      "Epoch 3/5\n",
      "5396/5396 [==============================] - 19s 4ms/step - loss: 0.0278 - accuracy: 0.6992\n",
      "Epoch 4/5\n",
      "5396/5396 [==============================] - 19s 4ms/step - loss: 0.0277 - accuracy: 0.7001\n",
      "Epoch 5/5\n",
      "5396/5396 [==============================] - 19s 4ms/step - loss: 0.0277 - accuracy: 0.7013\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0091 - accuracy: 0.9510\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0090 - accuracy: 0.9516\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0107 - accuracy: 0.9343\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0103 - accuracy: 0.9348\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0103 - accuracy: 0.9348\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0103 - accuracy: 0.9348\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0103 - accuracy: 0.9347\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0107 - accuracy: 0.9332\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0098 - accuracy: 0.9394\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0099 - accuracy: 0.9394\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0102 - accuracy: 0.9392\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0109 - accuracy: 0.9394\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0090 - accuracy: 0.9490\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0088 - accuracy: 0.9491\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0088 - accuracy: 0.9491\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0089 - accuracy: 0.9491\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0090 - accuracy: 0.9491\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0151 - accuracy: 0.8987\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0147 - accuracy: 0.8993\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0148 - accuracy: 0.8993\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0148 - accuracy: 0.8993\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0149 - accuracy: 0.8993\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0276 - accuracy: 0.6980\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0273 - accuracy: 0.6954\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0272 - accuracy: 0.6956\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 47s 3ms/step - loss: 0.0273 - accuracy: 0.6939\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0272 - accuracy: 0.6951\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0299 - accuracy: 0.6645\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0297 - accuracy: 0.6760\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0296 - accuracy: 0.6780\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0295 - accuracy: 0.6795\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 48s 4ms/step - loss: 0.0295 - accuracy: 0.6796\n",
      "3399/3399 [==============================] - 3s 984us/step\n",
      "3399/3399 [==============================] - 3s 997us/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 3s 998us/step\n",
      "3399/3399 [==============================] - 3s 984us/step\n",
      "3399/3399 [==============================] - 3s 964us/step\n",
      "3399/3399 [==============================] - 3s 1ms/step\n",
      "3399/3399 [==============================] - 3s 1000us/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 3s 876us/step\n",
      "3399/3399 [==============================] - 3s 939us/step\n",
      "3399/3399 [==============================] - 3s 979us/step\n",
      "3399/3399 [==============================] - 3s 926us/step\n",
      "3399/3399 [==============================] - 3s 944us/step\n",
      "3399/3399 [==============================] - 3s 949us/step\n",
      "3399/3399 [==============================] - 3s 995us/step\n",
      "3399/3399 [==============================] - 3s 971us/step\n",
      "3399/3399 [==============================] - 3s 956us/step\n",
      "3399/3399 [==============================] - 3s 972us/step\n",
      "3399/3399 [==============================] - 3s 940us/step\n",
      "3399/3399 [==============================] - 3s 992us/step\n",
      "3399/3399 [==============================] - 3s 944us/step\n",
      "3399/3399 [==============================] - 3s 958us/step\n",
      "3399/3399 [==============================] - 3s 958us/step\n",
      "3399/3399 [==============================] - 3s 972us/step\n",
      "3399/3399 [==============================] - 3s 951us/step\n",
      "3399/3399 [==============================] - 3s 943us/step\n",
      "3399/3399 [==============================] - 3s 954us/step\n",
      "3399/3399 [==============================] - 3s 963us/step\n",
      "3399/3399 [==============================] - 3s 923us/step\n",
      "3399/3399 [==============================] - 3s 942us/step\n",
      "3399/3399 [==============================] - 3s 951us/step\n",
      "3399/3399 [==============================] - 3s 955us/step\n",
      "3399/3399 [==============================] - 3s 984us/step\n",
      "Train Time: 3078.4881093502045 | Test Time: 151.0650110244751\n"
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=10, batch_size=32, verbose=0)\n",
    "classifier = CalibratedLabelRanking(classifier=Keras(DropoutModel, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction)\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "clr_exp.append(example_based(trues, preds))\n",
    "clr_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1255/1255 [==============================] - 6s 4ms/step - loss: 0.0319 - accuracy: 0.6083\n",
      "Epoch 2/5\n",
      "1255/1255 [==============================] - 5s 4ms/step - loss: 0.0316 - accuracy: 0.6181\n",
      "Epoch 3/5\n",
      "1255/1255 [==============================] - 5s 4ms/step - loss: 0.0315 - accuracy: 0.6212\n",
      "Epoch 4/5\n",
      "1255/1255 [==============================] - 5s 4ms/step - loss: 0.0316 - accuracy: 0.6171\n",
      "Epoch 5/5\n",
      "1255/1255 [==============================] - 4s 4ms/step - loss: 0.0315 - accuracy: 0.6182\n",
      "Epoch 1/5\n",
      "1235/1235 [==============================] - 5s 4ms/step - loss: 0.0319 - accuracy: 0.6007\n",
      "Epoch 2/5\n",
      "1235/1235 [==============================] - 5s 4ms/step - loss: 0.0316 - accuracy: 0.6144\n",
      "Epoch 3/5\n",
      "1235/1235 [==============================] - 5s 4ms/step - loss: 0.0315 - accuracy: 0.6165\n",
      "Epoch 4/5\n",
      "1235/1235 [==============================] - 5s 4ms/step - loss: 0.0315 - accuracy: 0.6205\n",
      "Epoch 5/5\n",
      "1235/1235 [==============================] - 5s 4ms/step - loss: 0.0314 - accuracy: 0.6186\n",
      "Epoch 1/5\n",
      "1157/1157 [==============================] - 5s 4ms/step - loss: 0.0330 - accuracy: 0.5516\n",
      "Epoch 2/5\n",
      "1157/1157 [==============================] - 4s 4ms/step - loss: 0.0328 - accuracy: 0.5686\n",
      "Epoch 3/5\n",
      "1157/1157 [==============================] - 4s 4ms/step - loss: 0.0327 - accuracy: 0.5724\n",
      "Epoch 4/5\n",
      "1157/1157 [==============================] - 4s 4ms/step - loss: 0.0327 - accuracy: 0.5699\n",
      "Epoch 5/5\n",
      "1157/1157 [==============================] - 4s 4ms/step - loss: 0.0327 - accuracy: 0.5753\n",
      "Epoch 1/5\n",
      "1730/1730 [==============================] - 8s 4ms/step - loss: 0.0284 - accuracy: 0.7086\n",
      "Epoch 2/5\n",
      "1730/1730 [==============================] - 7s 4ms/step - loss: 0.0280 - accuracy: 0.7165\n",
      "Epoch 3/5\n",
      "1730/1730 [==============================] - 7s 4ms/step - loss: 0.0280 - accuracy: 0.7148\n",
      "Epoch 4/5\n",
      "1730/1730 [==============================] - 7s 4ms/step - loss: 0.0279 - accuracy: 0.7163\n",
      "Epoch 5/5\n",
      "1730/1730 [==============================] - 7s 4ms/step - loss: 0.0279 - accuracy: 0.7173\n",
      "Epoch 1/5\n",
      "5079/5079 [==============================] - 21s 4ms/step - loss: 0.0092 - accuracy: 0.9564\n",
      "Epoch 2/5\n",
      "5079/5079 [==============================] - 20s 4ms/step - loss: 0.0085 - accuracy: 0.9565\n",
      "Epoch 3/5\n",
      "5079/5079 [==============================] - 20s 4ms/step - loss: 0.0085 - accuracy: 0.9565\n",
      "Epoch 4/5\n",
      "5079/5079 [==============================] - 20s 4ms/step - loss: 0.0085 - accuracy: 0.9565\n",
      "Epoch 5/5\n",
      "5079/5079 [==============================] - 20s 4ms/step - loss: 0.0086 - accuracy: 0.9565\n",
      "Epoch 1/5\n",
      "5741/5741 [==============================] - 24s 4ms/step - loss: 0.0093 - accuracy: 0.9507\n",
      "Epoch 2/5\n",
      "5741/5741 [==============================] - 24s 4ms/step - loss: 0.0088 - accuracy: 0.9509\n",
      "Epoch 3/5\n",
      "5741/5741 [==============================] - 23s 4ms/step - loss: 0.0089 - accuracy: 0.9509\n",
      "Epoch 4/5\n",
      "5741/5741 [==============================] - 24s 4ms/step - loss: 0.0088 - accuracy: 0.9509\n",
      "Epoch 5/5\n",
      "5741/5741 [==============================] - 24s 4ms/step - loss: 0.0088 - accuracy: 0.9509\n",
      "Epoch 1/5\n",
      "864/864 [==============================] - 4s 4ms/step - loss: 0.0331 - accuracy: 0.5472\n",
      "Epoch 2/5\n",
      "864/864 [==============================] - 3s 4ms/step - loss: 0.0329 - accuracy: 0.5650\n",
      "Epoch 3/5\n",
      "864/864 [==============================] - 3s 4ms/step - loss: 0.0328 - accuracy: 0.5688\n",
      "Epoch 4/5\n",
      "864/864 [==============================] - 3s 4ms/step - loss: 0.0328 - accuracy: 0.5673\n",
      "Epoch 5/5\n",
      "864/864 [==============================] - 3s 4ms/step - loss: 0.0328 - accuracy: 0.5673\n",
      "Epoch 1/5\n",
      "1228/1228 [==============================] - 5s 4ms/step - loss: 0.0326 - accuracy: 0.5706\n",
      "Epoch 2/5\n",
      "1228/1228 [==============================] - 5s 4ms/step - loss: 0.0322 - accuracy: 0.5939\n",
      "Epoch 3/5\n",
      "1228/1228 [==============================] - 5s 4ms/step - loss: 0.0321 - accuracy: 0.5949\n",
      "Epoch 4/5\n",
      "1228/1228 [==============================] - 5s 4ms/step - loss: 0.0320 - accuracy: 0.5982\n",
      "Epoch 5/5\n",
      "1228/1228 [==============================] - 5s 4ms/step - loss: 0.0320 - accuracy: 0.6006\n",
      "Epoch 1/5\n",
      "1915/1915 [==============================] - 8s 4ms/step - loss: 0.0303 - accuracy: 0.6483\n",
      "Epoch 2/5\n",
      "1915/1915 [==============================] - 7s 4ms/step - loss: 0.0297 - accuracy: 0.6548\n",
      "Epoch 3/5\n",
      "1915/1915 [==============================] - 8s 4ms/step - loss: 0.0296 - accuracy: 0.6588\n",
      "Epoch 4/5\n",
      "1915/1915 [==============================] - 8s 4ms/step - loss: 0.0296 - accuracy: 0.6577\n",
      "Epoch 5/5\n",
      "1915/1915 [==============================] - 8s 4ms/step - loss: 0.0296 - accuracy: 0.6593\n",
      "Epoch 1/5\n",
      "4962/4962 [==============================] - 21s 4ms/step - loss: 0.0106 - accuracy: 0.9442\n",
      "Epoch 2/5\n",
      "4962/4962 [==============================] - 20s 4ms/step - loss: 0.0100 - accuracy: 0.9442\n",
      "Epoch 3/5\n",
      "4962/4962 [==============================] - 20s 4ms/step - loss: 0.0100 - accuracy: 0.9442\n",
      "Epoch 4/5\n",
      "4962/4962 [==============================] - 19s 4ms/step - loss: 0.0100 - accuracy: 0.9442\n",
      "Epoch 5/5\n",
      "4962/4962 [==============================] - 19s 4ms/step - loss: 0.0101 - accuracy: 0.9442\n",
      "Epoch 1/5\n",
      "5747/5747 [==============================] - 22s 4ms/step - loss: 0.0111 - accuracy: 0.9306\n",
      "Epoch 2/5\n",
      "5747/5747 [==============================] - 22s 4ms/step - loss: 0.0106 - accuracy: 0.9306\n",
      "Epoch 3/5\n",
      "5747/5747 [==============================] - 21s 4ms/step - loss: 0.0106 - accuracy: 0.9306\n",
      "Epoch 4/5\n",
      "5747/5747 [==============================] - 23s 4ms/step - loss: 0.0107 - accuracy: 0.9306\n",
      "Epoch 5/5\n",
      "5747/5747 [==============================] - 23s 4ms/step - loss: 0.0107 - accuracy: 0.9306\n",
      "Epoch 1/5\n",
      "1123/1123 [==============================] - 5s 4ms/step - loss: 0.0325 - accuracy: 0.5834\n",
      "Epoch 2/5\n",
      "1123/1123 [==============================] - 4s 4ms/step - loss: 0.0322 - accuracy: 0.5922\n",
      "Epoch 3/5\n",
      "1123/1123 [==============================] - 4s 4ms/step - loss: 0.0322 - accuracy: 0.5941\n",
      "Epoch 4/5\n",
      "1123/1123 [==============================] - 4s 4ms/step - loss: 0.0322 - accuracy: 0.5942\n",
      "Epoch 5/5\n",
      "1123/1123 [==============================] - 5s 4ms/step - loss: 0.0322 - accuracy: 0.5924\n",
      "Epoch 1/5\n",
      "1886/1886 [==============================] - 8s 4ms/step - loss: 0.0293 - accuracy: 0.6742\n",
      "Epoch 2/5\n",
      "1886/1886 [==============================] - 7s 4ms/step - loss: 0.0287 - accuracy: 0.6865\n",
      "Epoch 3/5\n",
      "1886/1886 [==============================] - 7s 4ms/step - loss: 0.0287 - accuracy: 0.6840\n",
      "Epoch 4/5\n",
      "1886/1886 [==============================] - 7s 4ms/step - loss: 0.0287 - accuracy: 0.6855\n",
      "Epoch 5/5\n",
      "1886/1886 [==============================] - 7s 4ms/step - loss: 0.0287 - accuracy: 0.6835\n",
      "Epoch 1/5\n",
      "5041/5041 [==============================] - 20s 4ms/step - loss: 0.0107 - accuracy: 0.9435\n",
      "Epoch 2/5\n",
      "5041/5041 [==============================] - 20s 4ms/step - loss: 0.0100 - accuracy: 0.9435\n",
      "Epoch 3/5\n",
      "5041/5041 [==============================] - 19s 4ms/step - loss: 0.0100 - accuracy: 0.9435\n",
      "Epoch 4/5\n",
      "5041/5041 [==============================] - 18s 4ms/step - loss: 0.0100 - accuracy: 0.9435\n",
      "Epoch 5/5\n",
      "5041/5041 [==============================] - 19s 4ms/step - loss: 0.0100 - accuracy: 0.9435\n",
      "Epoch 1/5\n",
      "5775/5775 [==============================] - 23s 4ms/step - loss: 0.0116 - accuracy: 0.9259\n",
      "Epoch 2/5\n",
      "5775/5775 [==============================] - 23s 4ms/step - loss: 0.0103 - accuracy: 0.9340\n",
      "Epoch 3/5\n",
      "5775/5775 [==============================] - 23s 4ms/step - loss: 0.0103 - accuracy: 0.9339\n",
      "Epoch 4/5\n",
      "5775/5775 [==============================] - 22s 4ms/step - loss: 0.0104 - accuracy: 0.9340\n",
      "Epoch 5/5\n",
      "5775/5775 [==============================] - 22s 4ms/step - loss: 0.0103 - accuracy: 0.9340\n",
      "Epoch 1/5\n",
      "1799/1799 [==============================] - 7s 4ms/step - loss: 0.0288 - accuracy: 0.6947\n",
      "Epoch 2/5\n",
      "1799/1799 [==============================] - 7s 4ms/step - loss: 0.0284 - accuracy: 0.6992\n",
      "Epoch 3/5\n",
      "1799/1799 [==============================] - 7s 4ms/step - loss: 0.0284 - accuracy: 0.6994\n",
      "Epoch 4/5\n",
      "1799/1799 [==============================] - 7s 4ms/step - loss: 0.0283 - accuracy: 0.6999\n",
      "Epoch 5/5\n",
      "1799/1799 [==============================] - 7s 4ms/step - loss: 0.0283 - accuracy: 0.7004\n",
      "Epoch 1/5\n",
      "5084/5084 [==============================] - 21s 4ms/step - loss: 0.0097 - accuracy: 0.9526\n",
      "Epoch 2/5\n",
      "5084/5084 [==============================] - 21s 4ms/step - loss: 0.0091 - accuracy: 0.9526\n",
      "Epoch 3/5\n",
      "5084/5084 [==============================] - 21s 4ms/step - loss: 0.0091 - accuracy: 0.9526\n",
      "Epoch 4/5\n",
      "5084/5084 [==============================] - 19s 4ms/step - loss: 0.0092 - accuracy: 0.9526\n",
      "Epoch 5/5\n",
      "5084/5084 [==============================] - 19s 4ms/step - loss: 0.0091 - accuracy: 0.9526\n",
      "Epoch 1/5\n",
      "5737/5737 [==============================] - 26s 4ms/step - loss: 0.0100 - accuracy: 0.9481\n",
      "Epoch 2/5\n",
      "5737/5737 [==============================] - 23s 4ms/step - loss: 0.0092 - accuracy: 0.9482\n",
      "Epoch 3/5\n",
      "5737/5737 [==============================] - 23s 4ms/step - loss: 0.0094 - accuracy: 0.9482\n",
      "Epoch 4/5\n",
      "5737/5737 [==============================] - 22s 4ms/step - loss: 0.0095 - accuracy: 0.9482\n",
      "Epoch 5/5\n",
      "5737/5737 [==============================] - 23s 4ms/step - loss: 0.0094 - accuracy: 0.9482\n",
      "Epoch 1/5\n",
      "5078/5078 [==============================] - 21s 4ms/step - loss: 0.0172 - accuracy: 0.8862\n",
      "Epoch 2/5\n",
      "5078/5078 [==============================] - 20s 4ms/step - loss: 0.0168 - accuracy: 0.8865\n",
      "Epoch 3/5\n",
      "5078/5078 [==============================] - 21s 4ms/step - loss: 0.0168 - accuracy: 0.8865\n",
      "Epoch 4/5\n",
      "5078/5078 [==============================] - 21s 4ms/step - loss: 0.0169 - accuracy: 0.8865\n",
      "Epoch 5/5\n",
      "5078/5078 [==============================] - 21s 4ms/step - loss: 0.0170 - accuracy: 0.8865\n",
      "Epoch 1/5\n",
      "5882/5882 [==============================] - 25s 4ms/step - loss: 0.0162 - accuracy: 0.8795\n",
      "Epoch 2/5\n",
      "5882/5882 [==============================] - 24s 4ms/step - loss: 0.0160 - accuracy: 0.8796\n",
      "Epoch 3/5\n",
      "5882/5882 [==============================] - 24s 4ms/step - loss: 0.0159 - accuracy: 0.8796\n",
      "Epoch 4/5\n",
      "5882/5882 [==============================] - 24s 4ms/step - loss: 0.0159 - accuracy: 0.8797\n",
      "Epoch 5/5\n",
      "5882/5882 [==============================] - 23s 4ms/step - loss: 0.0159 - accuracy: 0.8798\n",
      "Epoch 1/5\n",
      "5396/5396 [==============================] - 22s 4ms/step - loss: 0.0282 - accuracy: 0.6732\n",
      "Epoch 2/5\n",
      "5396/5396 [==============================] - 22s 4ms/step - loss: 0.0278 - accuracy: 0.7015\n",
      "Epoch 3/5\n",
      "5396/5396 [==============================] - 22s 4ms/step - loss: 0.0276 - accuracy: 0.7145\n",
      "Epoch 4/5\n",
      "5396/5396 [==============================] - 22s 4ms/step - loss: 0.0275 - accuracy: 0.7145\n",
      "Epoch 5/5\n",
      "5396/5396 [==============================] - 22s 4ms/step - loss: 0.0276 - accuracy: 0.7137\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 54s 4ms/step - loss: 0.0091 - accuracy: 0.9513\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0087 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 54s 4ms/step - loss: 0.0087 - accuracy: 0.9516\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 56s 4ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 56s 4ms/step - loss: 0.0088 - accuracy: 0.9516\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 55s 4ms/step - loss: 0.0105 - accuracy: 0.9348\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 55s 4ms/step - loss: 0.0102 - accuracy: 0.9348\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 56s 4ms/step - loss: 0.0102 - accuracy: 0.9348\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0102 - accuracy: 0.9348\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 49s 4ms/step - loss: 0.0103 - accuracy: 0.9348\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0100 - accuracy: 0.9393\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0098 - accuracy: 0.9394\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0099 - accuracy: 0.9394\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 54s 4ms/step - loss: 0.0098 - accuracy: 0.9394\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 55s 4ms/step - loss: 0.0098 - accuracy: 0.9394\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 55s 4ms/step - loss: 0.0091 - accuracy: 0.9491\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0088 - accuracy: 0.9491\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0089 - accuracy: 0.9491\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0089 - accuracy: 0.9491\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0090 - accuracy: 0.9491\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 54s 4ms/step - loss: 0.0151 - accuracy: 0.8993\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0149 - accuracy: 0.8993\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0148 - accuracy: 0.8993\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0149 - accuracy: 0.8993\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0149 - accuracy: 0.8993\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0275 - accuracy: 0.6955\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0272 - accuracy: 0.7046\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0271 - accuracy: 0.7144\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 56s 4ms/step - loss: 0.0271 - accuracy: 0.7137\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 54s 4ms/step - loss: 0.0271 - accuracy: 0.7117\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 54s 4ms/step - loss: 0.0299 - accuracy: 0.6532\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 55s 4ms/step - loss: 0.0296 - accuracy: 0.6566\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0296 - accuracy: 0.6578\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 54s 4ms/step - loss: 0.0296 - accuracy: 0.6574\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 55s 4ms/step - loss: 0.0296 - accuracy: 0.6577\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 3s 991us/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 3s 965us/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "Train Time: 3348.6862330436707 | Test Time: 165.67239356040955\n"
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=10, batch_size=32, verbose=0)\n",
    "classifier = CalibratedLabelRanking(classifier=Keras(ProposedModel1, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction)\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "clr_exp.append(example_based(trues, preds))\n",
    "clr_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1255/1255 [==============================] - 5s 4ms/step - loss: 0.0317 - accuracy: 0.6125\n",
      "Epoch 2/5\n",
      "1255/1255 [==============================] - 5s 4ms/step - loss: 0.0314 - accuracy: 0.6194\n",
      "Epoch 3/5\n",
      "1255/1255 [==============================] - 5s 4ms/step - loss: 0.0314 - accuracy: 0.6199\n",
      "Epoch 4/5\n",
      "1255/1255 [==============================] - 5s 4ms/step - loss: 0.0314 - accuracy: 0.6214\n",
      "Epoch 5/5\n",
      "1255/1255 [==============================] - 5s 4ms/step - loss: 0.0314 - accuracy: 0.6181\n",
      "Epoch 1/5\n",
      "1235/1235 [==============================] - 5s 4ms/step - loss: 0.0316 - accuracy: 0.6092\n",
      "Epoch 2/5\n",
      "1235/1235 [==============================] - 4s 4ms/step - loss: 0.0313 - accuracy: 0.6169\n",
      "Epoch 3/5\n",
      "1235/1235 [==============================] - 4s 4ms/step - loss: 0.0312 - accuracy: 0.6188\n",
      "Epoch 4/5\n",
      "1235/1235 [==============================] - 5s 4ms/step - loss: 0.0311 - accuracy: 0.6207\n",
      "Epoch 5/5\n",
      "1235/1235 [==============================] - 5s 4ms/step - loss: 0.0311 - accuracy: 0.6240\n",
      "Epoch 1/5\n",
      "1157/1157 [==============================] - 5s 4ms/step - loss: 0.0328 - accuracy: 0.5665\n",
      "Epoch 2/5\n",
      "1157/1157 [==============================] - 4s 4ms/step - loss: 0.0326 - accuracy: 0.5800\n",
      "Epoch 3/5\n",
      "1157/1157 [==============================] - 4s 4ms/step - loss: 0.0325 - accuracy: 0.5792\n",
      "Epoch 4/5\n",
      "1157/1157 [==============================] - 4s 4ms/step - loss: 0.0325 - accuracy: 0.5829\n",
      "Epoch 5/5\n",
      "1157/1157 [==============================] - 4s 4ms/step - loss: 0.0324 - accuracy: 0.5833\n",
      "Epoch 1/5\n",
      "1730/1730 [==============================] - 8s 4ms/step - loss: 0.0281 - accuracy: 0.7134\n",
      "Epoch 2/5\n",
      "1730/1730 [==============================] - 7s 4ms/step - loss: 0.0278 - accuracy: 0.7146\n",
      "Epoch 3/5\n",
      "1730/1730 [==============================] - 7s 4ms/step - loss: 0.0276 - accuracy: 0.7174\n",
      "Epoch 4/5\n",
      "1730/1730 [==============================] - 7s 4ms/step - loss: 0.0276 - accuracy: 0.7177\n",
      "Epoch 5/5\n",
      "1730/1730 [==============================] - 7s 4ms/step - loss: 0.0276 - accuracy: 0.7155\n",
      "Epoch 1/5\n",
      "5079/5079 [==============================] - 21s 4ms/step - loss: 0.0090 - accuracy: 0.9565\n",
      "Epoch 2/5\n",
      "5079/5079 [==============================] - 21s 4ms/step - loss: 0.0085 - accuracy: 0.9565\n",
      "Epoch 3/5\n",
      "5079/5079 [==============================] - 19s 4ms/step - loss: 0.0085 - accuracy: 0.9565\n",
      "Epoch 4/5\n",
      "5079/5079 [==============================] - 19s 4ms/step - loss: 0.0085 - accuracy: 0.9565\n",
      "Epoch 5/5\n",
      "5079/5079 [==============================] - 19s 4ms/step - loss: 0.0086 - accuracy: 0.9565\n",
      "Epoch 1/5\n",
      "5741/5741 [==============================] - 22s 4ms/step - loss: 0.0094 - accuracy: 0.9509\n",
      "Epoch 2/5\n",
      "5741/5741 [==============================] - 21s 4ms/step - loss: 0.0086 - accuracy: 0.9509\n",
      "Epoch 3/5\n",
      "5741/5741 [==============================] - 21s 4ms/step - loss: 0.0086 - accuracy: 0.9509\n",
      "Epoch 4/5\n",
      "5741/5741 [==============================] - 21s 4ms/step - loss: 0.0086 - accuracy: 0.9509\n",
      "Epoch 5/5\n",
      "5741/5741 [==============================] - 21s 4ms/step - loss: 0.0086 - accuracy: 0.9509\n",
      "Epoch 1/5\n",
      "864/864 [==============================] - 4s 4ms/step - loss: 0.0329 - accuracy: 0.5641\n",
      "Epoch 2/5\n",
      "864/864 [==============================] - 3s 4ms/step - loss: 0.0327 - accuracy: 0.5771\n",
      "Epoch 3/5\n",
      "864/864 [==============================] - 3s 4ms/step - loss: 0.0327 - accuracy: 0.5793\n",
      "Epoch 4/5\n",
      "864/864 [==============================] - 3s 4ms/step - loss: 0.0326 - accuracy: 0.5836\n",
      "Epoch 5/5\n",
      "864/864 [==============================] - 3s 4ms/step - loss: 0.0325 - accuracy: 0.5839\n",
      "Epoch 1/5\n",
      "1228/1228 [==============================] - 5s 4ms/step - loss: 0.0322 - accuracy: 0.5933\n",
      "Epoch 2/5\n",
      "1228/1228 [==============================] - 4s 4ms/step - loss: 0.0319 - accuracy: 0.6046\n",
      "Epoch 3/5\n",
      "1228/1228 [==============================] - 4s 4ms/step - loss: 0.0318 - accuracy: 0.6084\n",
      "Epoch 4/5\n",
      "1228/1228 [==============================] - 4s 4ms/step - loss: 0.0318 - accuracy: 0.6075\n",
      "Epoch 5/5\n",
      "1228/1228 [==============================] - 5s 4ms/step - loss: 0.0318 - accuracy: 0.6073\n",
      "Epoch 1/5\n",
      "1915/1915 [==============================] - 7s 4ms/step - loss: 0.0300 - accuracy: 0.6561\n",
      "Epoch 2/5\n",
      "1915/1915 [==============================] - 7s 4ms/step - loss: 0.0295 - accuracy: 0.6668\n",
      "Epoch 3/5\n",
      "1915/1915 [==============================] - 7s 4ms/step - loss: 0.0295 - accuracy: 0.6658\n",
      "Epoch 4/5\n",
      "1915/1915 [==============================] - 7s 4ms/step - loss: 0.0294 - accuracy: 0.6664\n",
      "Epoch 5/5\n",
      "1915/1915 [==============================] - 7s 4ms/step - loss: 0.0294 - accuracy: 0.6652\n",
      "Epoch 1/5\n",
      "4962/4962 [==============================] - 19s 4ms/step - loss: 0.0104 - accuracy: 0.9442\n",
      "Epoch 2/5\n",
      "4962/4962 [==============================] - 19s 4ms/step - loss: 0.0100 - accuracy: 0.9442\n",
      "Epoch 3/5\n",
      "4962/4962 [==============================] - 20s 4ms/step - loss: 0.0100 - accuracy: 0.9442\n",
      "Epoch 4/5\n",
      "4962/4962 [==============================] - 19s 4ms/step - loss: 0.0100 - accuracy: 0.9442\n",
      "Epoch 5/5\n",
      "4962/4962 [==============================] - 19s 4ms/step - loss: 0.0100 - accuracy: 0.9442\n",
      "Epoch 1/5\n",
      "5747/5747 [==============================] - 22s 4ms/step - loss: 0.0112 - accuracy: 0.9305\n",
      "Epoch 2/5\n",
      "5747/5747 [==============================] - 22s 4ms/step - loss: 0.0105 - accuracy: 0.9306\n",
      "Epoch 3/5\n",
      "5747/5747 [==============================] - 23s 4ms/step - loss: 0.0105 - accuracy: 0.9306\n",
      "Epoch 4/5\n",
      "5747/5747 [==============================] - 23s 4ms/step - loss: 0.0102 - accuracy: 0.9306\n",
      "Epoch 5/5\n",
      "5747/5747 [==============================] - 21s 4ms/step - loss: 0.0102 - accuracy: 0.9306\n",
      "Epoch 1/5\n",
      "1123/1123 [==============================] - 5s 4ms/step - loss: 0.0323 - accuracy: 0.5887\n",
      "Epoch 2/5\n",
      "1123/1123 [==============================] - 4s 4ms/step - loss: 0.0321 - accuracy: 0.5965\n",
      "Epoch 3/5\n",
      "1123/1123 [==============================] - 4s 4ms/step - loss: 0.0320 - accuracy: 0.5962\n",
      "Epoch 4/5\n",
      "1123/1123 [==============================] - 4s 4ms/step - loss: 0.0320 - accuracy: 0.5993\n",
      "Epoch 5/5\n",
      "1123/1123 [==============================] - 4s 4ms/step - loss: 0.0319 - accuracy: 0.5991\n",
      "Epoch 1/5\n",
      "1886/1886 [==============================] - 8s 4ms/step - loss: 0.0291 - accuracy: 0.6730\n",
      "Epoch 2/5\n",
      "1886/1886 [==============================] - 8s 4ms/step - loss: 0.0286 - accuracy: 0.6851\n",
      "Epoch 3/5\n",
      "1886/1886 [==============================] - 8s 4ms/step - loss: 0.0285 - accuracy: 0.6847\n",
      "Epoch 4/5\n",
      "1886/1886 [==============================] - 8s 4ms/step - loss: 0.0285 - accuracy: 0.6857\n",
      "Epoch 5/5\n",
      "1886/1886 [==============================] - 8s 4ms/step - loss: 0.0285 - accuracy: 0.6857\n",
      "Epoch 1/5\n",
      "5041/5041 [==============================] - 19s 4ms/step - loss: 0.0108 - accuracy: 0.9433\n",
      "Epoch 2/5\n",
      "5041/5041 [==============================] - 18s 4ms/step - loss: 0.0099 - accuracy: 0.9435\n",
      "Epoch 3/5\n",
      "5041/5041 [==============================] - 19s 4ms/step - loss: 0.0100 - accuracy: 0.9435\n",
      "Epoch 4/5\n",
      "5041/5041 [==============================] - 19s 4ms/step - loss: 0.0100 - accuracy: 0.9435\n",
      "Epoch 5/5\n",
      "5041/5041 [==============================] - 19s 4ms/step - loss: 0.0100 - accuracy: 0.9435\n",
      "Epoch 1/5\n",
      "5775/5775 [==============================] - 22s 4ms/step - loss: 0.0106 - accuracy: 0.9335\n",
      "Epoch 2/5\n",
      "5775/5775 [==============================] - 22s 4ms/step - loss: 0.0100 - accuracy: 0.9340\n",
      "Epoch 3/5\n",
      "5775/5775 [==============================] - 21s 4ms/step - loss: 0.0101 - accuracy: 0.9340\n",
      "Epoch 4/5\n",
      "5775/5775 [==============================] - 22s 4ms/step - loss: 0.0102 - accuracy: 0.9340\n",
      "Epoch 5/5\n",
      "5775/5775 [==============================] - 23s 4ms/step - loss: 0.0105 - accuracy: 0.9340\n",
      "Epoch 1/5\n",
      "1799/1799 [==============================] - 7s 4ms/step - loss: 0.0286 - accuracy: 0.6958\n",
      "Epoch 2/5\n",
      "1799/1799 [==============================] - 7s 4ms/step - loss: 0.0284 - accuracy: 0.6976\n",
      "Epoch 3/5\n",
      "1799/1799 [==============================] - 6s 4ms/step - loss: 0.0282 - accuracy: 0.7005\n",
      "Epoch 4/5\n",
      "1799/1799 [==============================] - 6s 4ms/step - loss: 0.0282 - accuracy: 0.7006\n",
      "Epoch 5/5\n",
      "1799/1799 [==============================] - 7s 4ms/step - loss: 0.0282 - accuracy: 0.7000\n",
      "Epoch 1/5\n",
      "5084/5084 [==============================] - 20s 4ms/step - loss: 0.0097 - accuracy: 0.9521\n",
      "Epoch 2/5\n",
      "5084/5084 [==============================] - 19s 4ms/step - loss: 0.0091 - accuracy: 0.9526\n",
      "Epoch 3/5\n",
      "5084/5084 [==============================] - 19s 4ms/step - loss: 0.0091 - accuracy: 0.9526\n",
      "Epoch 4/5\n",
      "5084/5084 [==============================] - 20s 4ms/step - loss: 0.0091 - accuracy: 0.9526\n",
      "Epoch 5/5\n",
      "5084/5084 [==============================] - 20s 4ms/step - loss: 0.0091 - accuracy: 0.9526\n",
      "Epoch 1/5\n",
      "5737/5737 [==============================] - 23s 4ms/step - loss: 0.0098 - accuracy: 0.9482\n",
      "Epoch 2/5\n",
      "5737/5737 [==============================] - 22s 4ms/step - loss: 0.0090 - accuracy: 0.9482\n",
      "Epoch 3/5\n",
      "5737/5737 [==============================] - 21s 4ms/step - loss: 0.0091 - accuracy: 0.9482\n",
      "Epoch 4/5\n",
      "5737/5737 [==============================] - 23s 4ms/step - loss: 0.0093 - accuracy: 0.9482\n",
      "Epoch 5/5\n",
      "5737/5737 [==============================] - 22s 4ms/step - loss: 0.0093 - accuracy: 0.9482\n",
      "Epoch 1/5\n",
      "5078/5078 [==============================] - 21s 4ms/step - loss: 0.0175 - accuracy: 0.8855\n",
      "Epoch 2/5\n",
      "5078/5078 [==============================] - 21s 4ms/step - loss: 0.0171 - accuracy: 0.8865\n",
      "Epoch 3/5\n",
      "5078/5078 [==============================] - 21s 4ms/step - loss: 0.0171 - accuracy: 0.8865\n",
      "Epoch 4/5\n",
      "5078/5078 [==============================] - 21s 4ms/step - loss: 0.0171 - accuracy: 0.8865\n",
      "Epoch 5/5\n",
      "5078/5078 [==============================] - 21s 4ms/step - loss: 0.0171 - accuracy: 0.8865\n",
      "Epoch 1/5\n",
      "5882/5882 [==============================] - 23s 4ms/step - loss: 0.0162 - accuracy: 0.8796\n",
      "Epoch 2/5\n",
      "5882/5882 [==============================] - 24s 4ms/step - loss: 0.0154 - accuracy: 0.8796\n",
      "Epoch 3/5\n",
      "5882/5882 [==============================] - 24s 4ms/step - loss: 0.0155 - accuracy: 0.8796\n",
      "Epoch 4/5\n",
      "5882/5882 [==============================] - 24s 4ms/step - loss: 0.0155 - accuracy: 0.8796\n",
      "Epoch 5/5\n",
      "5882/5882 [==============================] - 22s 4ms/step - loss: 0.0155 - accuracy: 0.8796\n",
      "Epoch 1/5\n",
      "5396/5396 [==============================] - 22s 4ms/step - loss: 0.0271 - accuracy: 0.7109\n",
      "Epoch 2/5\n",
      "5396/5396 [==============================] - 22s 4ms/step - loss: 0.0268 - accuracy: 0.7160\n",
      "Epoch 3/5\n",
      "5396/5396 [==============================] - 23s 4ms/step - loss: 0.0267 - accuracy: 0.7174\n",
      "Epoch 4/5\n",
      "5396/5396 [==============================] - 22s 4ms/step - loss: 0.0267 - accuracy: 0.7169\n",
      "Epoch 5/5\n",
      "5396/5396 [==============================] - 21s 4ms/step - loss: 0.0267 - accuracy: 0.7168\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 56s 4ms/step - loss: 0.0088 - accuracy: 0.9514\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 57s 4ms/step - loss: 0.0085 - accuracy: 0.9516\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 58s 4ms/step - loss: 0.0086 - accuracy: 0.9516\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 56s 4ms/step - loss: 0.0087 - accuracy: 0.9516\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 56s 4ms/step - loss: 0.0087 - accuracy: 0.9516\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0103 - accuracy: 0.9347\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0099 - accuracy: 0.9348\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0099 - accuracy: 0.9348\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0099 - accuracy: 0.9348\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0099 - accuracy: 0.9348\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0098 - accuracy: 0.9393\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0094 - accuracy: 0.9394\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0095 - accuracy: 0.9394\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0095 - accuracy: 0.9394\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0097 - accuracy: 0.9394\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0088 - accuracy: 0.9490\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0086 - accuracy: 0.9491\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0086 - accuracy: 0.9491\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0086 - accuracy: 0.9491\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0087 - accuracy: 0.9491\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0151 - accuracy: 0.8993\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0145 - accuracy: 0.8993\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 54s 4ms/step - loss: 0.0145 - accuracy: 0.8993\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0145 - accuracy: 0.8993\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0145 - accuracy: 0.8993\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0270 - accuracy: 0.7002\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0263 - accuracy: 0.7219\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 50s 4ms/step - loss: 0.0262 - accuracy: 0.7244\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0262 - accuracy: 0.7245\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 52s 4ms/step - loss: 0.0262 - accuracy: 0.7243\n",
      "Epoch 1/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0291 - accuracy: 0.6798\n",
      "Epoch 2/5\n",
      "13595/13595 [==============================] - 51s 4ms/step - loss: 0.0289 - accuracy: 0.6840\n",
      "Epoch 3/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0288 - accuracy: 0.6840\n",
      "Epoch 4/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0288 - accuracy: 0.6843\n",
      "Epoch 5/5\n",
      "13595/13595 [==============================] - 53s 4ms/step - loss: 0.0288 - accuracy: 0.6847\n",
      "3399/3399 [==============================] - 3s 986us/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "3399/3399 [==============================] - 4s 1ms/step\n",
      "Train Time: 3267.1702864170074 | Test Time: 162.14694809913635\n"
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=10, batch_size=32, verbose=0)\n",
    "classifier = CalibratedLabelRanking(classifier=Keras(ProposedModel2, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction)\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "clr_exp.append(example_based(trues, preds))\n",
    "clr_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3399/3399 [==============================] - 3s 882us/step\n",
      "3399/3399 [==============================] - 3s 962us/step\n",
      "3399/3399 [==============================] - 3s 955us/step\n",
      "3399/3399 [==============================] - 3s 963us/step\n",
      "3399/3399 [==============================] - 3s 945us/step\n",
      "3399/3399 [==============================] - 3s 936us/step\n",
      "3399/3399 [==============================] - 3s 942us/step\n",
      "3399/3399 [==============================] - 3s 958us/step\n",
      "3399/3399 [==============================] - 3s 924us/step\n",
      "3399/3399 [==============================] - 3s 913us/step\n",
      "3399/3399 [==============================] - 3s 926us/step\n",
      "3399/3399 [==============================] - 3s 919us/step\n",
      "3399/3399 [==============================] - 3s 942us/step\n",
      "3399/3399 [==============================] - 3s 953us/step\n",
      "3399/3399 [==============================] - 3s 939us/step\n",
      "3399/3399 [==============================] - 3s 935us/step\n",
      "3399/3399 [==============================] - 3s 935us/step\n",
      "3399/3399 [==============================] - 3s 946us/step\n",
      "3399/3399 [==============================] - 3s 924us/step\n",
      "3399/3399 [==============================] - 3s 964us/step\n",
      "3399/3399 [==============================] - 3s 954us/step\n",
      "3399/3399 [==============================] - 3s 954us/step\n",
      "3399/3399 [==============================] - 3s 938us/step\n",
      "3399/3399 [==============================] - 3s 927us/step\n",
      "3399/3399 [==============================] - 3s 934us/step\n",
      "3399/3399 [==============================] - 3s 925us/step\n",
      "3399/3399 [==============================] - 3s 936us/step\n",
      "3399/3399 [==============================] - 3s 970us/step\n",
      "3399/3399 [==============================] - 3s 963us/step\n",
      "3399/3399 [==============================] - 3s 977us/step\n",
      "3399/3399 [==============================] - 3s 983us/step\n",
      "3399/3399 [==============================] - 3s 953us/step\n",
      "3399/3399 [==============================] - 3s 965us/step\n",
      "3399/3399 [==============================] - 3s 935us/step\n",
      "3399/3399 [==============================] - 3s 978us/step\n",
      "Train Time: 2432.918788433075 | Test Time: 144.17081022262573\n"
     ]
    }
   ],
   "source": [
    "# KERAS_PARAMS = dict(epochs=10, batch_size=32, verbose=0)\n",
    "classifier = CalibratedLabelRanking(classifier=Keras(WeightedDropoutModel, False, KERAS_PARAMS))\n",
    "\n",
    "train_init = time.time()\n",
    "classifier.fit(x_train, y_train)\n",
    "train_time = time.time() - train_init\n",
    "\n",
    "test_init = time.time()\n",
    "prediction = classifier.predict(x_test)\n",
    "test_time = time.time() - test_init\n",
    "\n",
    "trues = torch.tensor(y_test)\n",
    "preds = torch.tensor(prediction)\n",
    "\n",
    "print(f\"Train Time: {train_time} | Test Time: {test_time}\")\n",
    "clr_exp.append(example_based(trues, preds))\n",
    "clr_label.append(label_based(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_name = 'clr2'\n",
    "with open(f'Results/{model_name}_exp.pkl', 'wb') as file:\n",
    "    pickle.dump(clr_exp, file)\n",
    "with open(f'Results/{model_name}_label.pkl', 'wb') as file:\n",
    "    pickle.dump(clr_label, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
