{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from torchmetrics.functional.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Subset Accuracy</th>\n",
       "      <th>Hamming Loss</th>\n",
       "      <th>Accuracy(exp)</th>\n",
       "      <th>Precision(exp)</th>\n",
       "      <th>Recall(exp)</th>\n",
       "      <th>F1-Score(exp)</th>\n",
       "      <th>One error</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Rangking Loss</th>\n",
       "      <th>...</th>\n",
       "      <th>Accuracy(micro)</th>\n",
       "      <th>Accuracy (macro)</th>\n",
       "      <th>Precision(micro)</th>\n",
       "      <th>Precision(macro)</th>\n",
       "      <th>Recall (micro)</th>\n",
       "      <th>Recall (macro)</th>\n",
       "      <th>F1-Score (micro)</th>\n",
       "      <th>F1-Score (macro)</th>\n",
       "      <th>AUC (micro)</th>\n",
       "      <th>AUC (macro)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Model, Subset Accuracy, Hamming Loss, Accuracy(exp), Precision(exp), Recall(exp), F1-Score(exp), One error, Coverage, Rangking Loss, Average precision, Accuracy(micro), Accuracy (macro), Precision(micro), Precision(macro), Recall (micro), Recall (macro), F1-Score (micro), F1-Score (macro), AUC (micro), AUC (macro)]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval = pd.DataFrame(columns=['Model','Subset Accuracy', 'Hamming Loss', 'Accuracy(exp)', 'Precision(exp)', 'Recall(exp)', 'F1-Score(exp)', 'One error', 'Coverage', 'Rangking Loss', 'Average precision', 'Accuracy(micro)', 'Accuracy (macro)', 'Precision(micro)', 'Precision(macro)', 'Recall (micro)', 'Recall (macro)', 'F1-Score (micro)', 'F1-Score (macro)', 'AUC (micro)', 'AUC (macro)'])\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ['knn', 'dt', 'ann', 'svm', 'rf', 'nb', 'dnn', 'ednn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Rifqi Fauzi\\AppData\\Local\\Temp\\ipykernel_18772\\607842060.py:117: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_eval = pd.concat([df_eval, df_temp], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "for k in model_name:\n",
    "    pkl = open(f'Results/{k}_fold_pred.pkl', 'rb')\n",
    "    pkl2 = open(f'Results/{k}_fold_test.pkl', 'rb')\n",
    "    fold_pred = pickle.load(pkl)\n",
    "    fold_test = pickle.load(pkl2)\n",
    "    acc_micro, acc_macro, prec_micro, prec_macro, recall_micro, recall_macro, f1_micro, f1_macro, auc_micro, auc_macro = [], [], [], [], [], [], [], [], [], []\n",
    "    type(fold_pred[0])\n",
    "    for i in range(10):\n",
    "        trues = torch.tensor(fold_test[i])\n",
    "        if isinstance(fold_pred[i], np.ndarray):\n",
    "            preds = torch.tensor(fold_pred[i])\n",
    "        else:\n",
    "            preds = torch.tensor(fold_pred[i].toarray())\n",
    "        acc_micro.append(multilabel_accuracy(preds, trues, num_labels=7, average='micro').item())\n",
    "        acc_macro.append(multilabel_accuracy(preds, trues, num_labels=7, average='macro').item())\n",
    "        prec_micro.append(multilabel_precision(preds, trues, num_labels=7, average='micro').item())\n",
    "        prec_macro.append(multilabel_precision(preds, trues, num_labels=7, average='macro').item())\n",
    "        recall_micro.append(multilabel_recall(preds, trues, num_labels=7, average='micro').item())\n",
    "        recall_macro.append(multilabel_recall(preds, trues, num_labels=7, average='macro').item())\n",
    "        f1_micro.append(multilabel_f1_score(preds, trues, num_labels=7, average='micro').item())\n",
    "        f1_macro.append(multilabel_f1_score(preds, trues, num_labels=7, average='macro').item())\n",
    "        auc_micro.append(multilabel_auroc(preds.float(), trues.long(), num_labels=7, average='micro').item())\n",
    "        auc_macro.append(multilabel_auroc(preds.float(), trues.long(), num_labels=7, average='macro').item())\n",
    "    import numpy as np\n",
    "    import scipy.io as sci\n",
    "\n",
    "    def findmax(outputs):\n",
    "        Max = -float(\"inf\")\n",
    "        index = 0\n",
    "        for i in range(outputs.shape[0]):\n",
    "            if outputs[i] > Max:\n",
    "                Max = outputs[i]\n",
    "                index = i\n",
    "        return Max, index\n",
    "\n",
    "    def OneError(outputs, test_target):\n",
    "        test_data_num = outputs.shape[0]\n",
    "        class_num = outputs.shape[1]\n",
    "        num = 0\n",
    "        one_error = 0\n",
    "        for i in range(test_data_num):\n",
    "            if sum(test_target[i]) != class_num and sum(test_target[i]) != 0:\n",
    "                Max, index = findmax(outputs[i])\n",
    "                num = num + 1\n",
    "                if test_target[i][index] != 1:\n",
    "                    one_error = one_error + 1\n",
    "        return one_error / num\n",
    "    def Accuracy(y_true, y_pred):\n",
    "        temp = 0\n",
    "        for i in range(y_true.shape[0]):\n",
    "            x = sum(np.logical_and(y_true[i], y_pred[i]))\n",
    "            y = sum(np.logical_or(y_true[i], y_pred[i]))\n",
    "            temp +=  x/y if not np.isnan(x/y) else 0\n",
    "        return temp / y_true.shape[0]\n",
    "\n",
    "    def Recall(y_true, y_pred):\n",
    "        temp = 0\n",
    "        for i in range(y_true.shape[0]):\n",
    "            if sum(y_true[i]) == 0:\n",
    "                continue\n",
    "            temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_true[i])\n",
    "        return temp/ y_true.shape[0]\n",
    "\n",
    "    def Precision(y_true, y_pred):\n",
    "        temp = 0\n",
    "        for i in range(y_true.shape[0]):\n",
    "            if sum(y_pred[i]) == 0:\n",
    "                continue\n",
    "            temp+= sum(np.logical_and(y_true[i], y_pred[i]))/ sum(y_pred[i])\n",
    "        return temp/ y_true.shape[0]\n",
    "    subset_acc, hamming_loss, acc_exp, prec_exp, recall_exp, f1_exp, one_error, coverage, ranking_loss, avg_precision = [], [], [], [], [], [], [], [], [], []\n",
    "    for i in range(10):\n",
    "        trues = torch.tensor(fold_test[i])\n",
    "        if isinstance(fold_pred[i], np.ndarray):\n",
    "            preds = torch.tensor(fold_pred[i])\n",
    "        else:\n",
    "            preds = torch.tensor(fold_pred[i].toarray())\n",
    "        subset_acc.append(multilabel_exact_match(preds, trues, num_labels=7).item())\n",
    "        hamming_loss.append(multilabel_hamming_distance(preds, trues, num_labels=7).item())\n",
    "        acc_exp.append(Accuracy(trues, preds))\n",
    "        prec_exp.append(Precision(trues, preds))\n",
    "        recall_exp.append(Recall(trues, preds))\n",
    "        f1_exp.append(multilabel_fbeta_score(preds, trues, beta=1.0, num_labels=7).item())\n",
    "        if isinstance(fold_pred[i], np.ndarray):\n",
    "            one_error.append(OneError(fold_pred[i], fold_test[i]))\n",
    "        else:\n",
    "            one_error.append(OneError(fold_pred[i].toarray(), fold_test[i]))  \n",
    "        coverage.append(multilabel_coverage_error(preds.float(), trues, num_labels=7).item())\n",
    "        ranking_loss.append(multilabel_ranking_loss(preds.float(), trues, num_labels=7).item())\n",
    "        avg_precision.append(multilabel_average_precision(preds.float(), trues.long(), num_labels=7).item())\n",
    "    np.mean(subset_acc)\n",
    "    dict_temp = {\n",
    "        'Model': f'BR-{k}',\n",
    "        'Subset Accuracy': np.mean(subset_acc), \n",
    "        'Hamming Loss': np.mean(hamming_loss), \n",
    "        'Accuracy(exp)': np.mean(acc_exp), \n",
    "        'Precision(exp)': np.mean(prec_exp), \n",
    "        'Recall(exp)': np.mean(recall_exp), \n",
    "        'F1-Score(exp)': np.mean(f1_exp), \n",
    "        'One error': np.mean(one_error), \n",
    "        'Coverage': np.mean(coverage), \n",
    "        'Rangking Loss': np.mean(ranking_loss), \n",
    "        'Average precision': np.mean(avg_precision), \n",
    "        'Accuracy(micro)': np.mean(acc_micro), \n",
    "        'Accuracy (macro)': np.mean(acc_macro), \n",
    "        'Precision(micro)': np.mean(prec_micro), \n",
    "        'Precision(macro)': np.mean(prec_macro), \n",
    "        'Recall (micro)': np.mean(recall_micro), \n",
    "        'Recall (macro)': np.mean(recall_macro), \n",
    "        'F1-Score (micro)': np.mean(f1_micro), \n",
    "        'F1-Score (macro)': np.mean(f1_macro), \n",
    "        'AUC (micro)': np.mean(auc_micro), \n",
    "        'AUC (macro)': np.mean(auc_macro)\n",
    "    }\n",
    "    dict_temp\n",
    "    df_temp = pd.DataFrame([dict_temp])\n",
    "    df_eval = pd.concat([df_eval, df_temp], ignore_index=True)\n",
    "    df_eval\n",
    "    df_eval.to_csv('Evaluation Results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
